{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ML-Challenge/week5-preprocessing-and-tunning/blob/master/L2.Hyperparameter%20Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building powerful machine learning models depends heavily on the set of hyperparameters used. But with increasingly complex models with lots of options, how do we efficiently find the best settings for our particular problem? In this lesson we will get practical experience in using some common methodologies for automated hyperparameter tuning in Python using Scikit Learn. These include Grid Search, Random Search & advanced optimization methodologies including Bayesian & Genetic algorithms. We will use a dataset predicting credit card defaults as we build skills to dramatically increase the efficiency and effectiveness of our machine learning model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:26.585604Z",
     "start_time": "2020-02-19T18:49:26.571604Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Download lesson datasets\n",
    "# Required if you're using Google Colab\n",
    "#!wget \"https://github.com/ML-Challenge/week5-preprocessing-and-tunning/raw/master/datasets.zip\"\n",
    "#!unzip -o datasets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:27.179977Z",
     "start_time": "2020-02-19T18:49:26.587602Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import utils\n",
    "# We'll be using this module throughout the lesson\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:27.837220Z",
     "start_time": "2020-02-19T18:49:27.182969Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# and setting the size of all plots.\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Hyperparameters and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this introductory section, we will learn the difference between hyperparameters and parameters. We will practice extracting and analyzing parameters, setting hyperparameter values for several popular machine learning algorithms. Along the way we will learn some best practice tips & tricks for choosing which hyperparameters to tune and what values to set & build learning curves to analyze our hyperparameter choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Introduction & 'Parameters'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Today algorithms are getting more and more complex, and so the number of hyperparameters to choose from increases. It becomes increasingly important to learn how to efficiently find optimal  combinations, as this search will likely take up a large portion of our time. Often it is quite easy to simply run Scikit Learn functions on the default settings or\n",
    "perhaps code from a tutorial or book without really digging under the hood. However, what lies underneath is of vital importance to good model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This lesson will use a dataset about credit card defaults. It contains a number of variables related to the demographics and financial history of a group of people. The target column shows whether or not they defaulted on their next loan payment. It has already been pre-processed and split ready to model. Note that at times we will take smaller samples to ensure we can run the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Parameters Overview**\n",
    "\n",
    "To understand hyperparameters, let's first start with parameters. What are parameters?\n",
    "\n",
    "Parameters are components of the final model that are learned through the modeling process. Crucially, we do not set these. We cannot set these. The algorithm discovers them through undertaking its steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To make this concrete, consider a simple logistic regression model. We create the estimator and fit to the data with default settings. \n",
    "\n",
    "```\n",
    "log_reg_clf = LogisticRegression()\n",
    "log_reg_clf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Since the logistic regression model is a linear model, we will get beta coefficients on our variables. These are found in the `coef_` property of our logistic regression object. \n",
    "\n",
    "```\n",
    "print(log_reg_clf.coef_)\n",
    "```\n",
    "\n",
    "However, if we print these out we can see it is a bit messy. Let us clean this up by creating a list of original variable names, zipping this up with the coefficients and formatting into a neat DataFrame for easy viewing. \n",
    "\n",
    "```\n",
    "# Get the original variable names\n",
    "original_variables = list(X_train.columns)\n",
    "\n",
    "# Zip together the names and coefficients\n",
    "zipped_together = list(zip(original_variables, log_reg_clf_coef_[0]))\n",
    "coefs = [list(x) for x in zipped_together]\n",
    "\n",
    "# Put into a DataFrame with column labels\n",
    "coefs = pd.DataFrames(coefs, columns=[\"Variable\", \"Coefficient\"]\n",
    "```\n",
    "\n",
    "We can now sort the DataFrame and print the top 3 results for brevity. \n",
    "\n",
    "```\n",
    "coefs.sort_values(by=['Coefficient'], axis=0, inplace=True, asceding=False)\n",
    "print(coefs.head(3))\n",
    "```\n",
    "\n",
    "| Variable | Coefficient |\n",
    "| -------- | ----------: |\n",
    "| PAY_0    | 0.000751    |\n",
    "| PAY_5    | 0.000438    |\n",
    "| PAY_4    | 0.000435    |\n",
    "\n",
    "Looking at the code above it is clearly that we didn't set the `PAY_0` to have a coefficient of 0.000751."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The coefficients are parameters because we did not set them ourselves and were learned during the modeling process. In our data, the `PAY` variables relate to how many months people have previously delayed their payments. We can see that having a high number of months of delayed payments, makes someone more likely to default next month. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Where to find Parameters**\n",
    "\n",
    "To know what parameters an algorithm will produce, we need to know a bit about the algorithm itself and how it works and consult the Scikit Learn documentation to see where the parameter is stored in the returned object.\n",
    "\n",
    "The parameters are found in the documentation for that particular algorithm under the 'Attributes' section, not the parameters section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Parameters in Random Forest**\n",
    "\n",
    "So what are the parameters in tree-based models that do not have linear coefficients? The parameters of this model are in the nodes of the trees used to build the model such as what feature was split on and at what value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To demonstrate, let us firstly build a random forest estimator & fit to our data, setting the `max_depth` to be quite\n",
    "low only for visualization purposes.\n",
    "\n",
    "```\n",
    "# A simple random forest estimator\n",
    "rf_clf = RandomForestClassifier(max_depth=2)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Then we can pull out a single tree, found in the random forest estimator `'estimators_'` attribute to visualize\n",
    "\n",
    "```\n",
    "# Pull out one tree from the forest\n",
    "chosen_tree = rf_clf.estimators_[7]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we see a graph of the nodes including the variables and values used in the splits:\n",
    "\n",
    "![Simple Tree](assets/2-1.png)\n",
    "\n",
    "We can see that the very first split was on the variable `PAY_4` and it sent samples left or right depending if they had a value above or below `1` for this variable.\n",
    "\n",
    "Again, looking at the above code it is clear that this decision was not set by us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Extracting Node Decisions**\n",
    "\n",
    "So how do we pull out the splits we saw here visually in a programmatic way? Let's say, the left, second-from-top node. The tree we pulled out is a Scikit Learn `'tree'` object so we can find the variable it split on by indexing into the `.feature` attribute of this tree and matching up with our `X_train` columns to get the name.\n",
    "\n",
    "```\n",
    "# Get the column it split on\n",
    "split_column = chose_tree.tree_.feature[1]\n",
    "split_column_name = X_train.columns[split_column]\n",
    "```\n",
    "\n",
    "The level used to split is then found in the `.threshold` attribute. And we can then print this out.\n",
    "\n",
    "```\n",
    "# Get the level it split on\n",
    "split_value = chosen_tree.tree_.threshold[1]\n",
    "\n",
    "print(f\"This node split on feature {split_column_name}, at a value of {split_value}\")\n",
    "```\n",
    "\n",
    "```\n",
    "\"This node split on feature PAY_0, at a value of 1.5\"\n",
    "```\n",
    "\n",
    "Let's do some examples to further explore the parameters of these models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Parameters in Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we have had a chance to explore what a parameter is, let us apply this knowledge. It is important to be able to review any new algorithm and identify which elements are parameters and hyperparameters.\n",
    "\n",
    "Which of the following is a parameter for the Scikit Learn logistic regression model?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "1. `n_jobs`\n",
    "2. `coef_`\n",
    "3. `class_weight`\n",
    "4. `LogisticRegression()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:27.853179Z",
     "start_time": "2020-02-19T18:49:27.840210Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pass 1,2,3 or 4 as argument\n",
    "utils.which_is_param()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Extracting a Logistic Regression parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are now going to practice extracting an important parameter of the logistic regression model. The logistic regression has a few other parameters we will not explore here, but they can be reviewed in the **scikit-learn.org** documentation for the `LogisticRegression()` module under 'Attributes'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This parameter is important for understanding the direction and magnitude of the effect the variables have on the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example we will extract the coefficient parameter (found in the `coef_` attribute), zip it up with the original column names, and see which variables had the largest positive effect on the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:27.884094Z",
     "start_time": "2020-02-19T18:49:27.857165Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "utils.credit_card.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:27.915035Z",
     "start_time": "2020-02-19T18:49:27.890078Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = utils.credit_card.drop('default payment next month', axis=1)\n",
    "y = utils.credit_card['default payment next month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:27.946924Z",
     "start_time": "2020-02-19T18:49:27.918004Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:27.977843Z",
     "start_time": "2020-02-19T18:49:27.949916Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:28.023720Z",
     "start_time": "2020-02-19T18:49:27.979839Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_clf = LogisticRegression(solver='liblinear', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:28.818596Z",
     "start_time": "2020-02-19T18:49:28.025714Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_reg_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:28.834552Z",
     "start_time": "2020-02-19T18:49:28.820590Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a list of original variable names from the training DataFrame\n",
    "original_variables = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:28.850509Z",
     "start_time": "2020-02-19T18:49:28.837543Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract the coefficients of the logistic regression estimator\n",
    "model_coefficients = log_reg_clf.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:28.881426Z",
     "start_time": "2020-02-19T18:49:28.853500Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a dataframe of the variables and coefficients & print it out\n",
    "coefficient_df = pd.DataFrame({\"Variable\" : original_variables, \"Coefficient\": model_coefficients})\n",
    "coefficient_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:28.896416Z",
     "start_time": "2020-02-19T18:49:28.883420Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Print out the top 3 positive variables\n",
    "top_three_df = coefficient_df.sort_values(by=\"Coefficient\", axis=0, ascending=False)[0:3]\n",
    "top_three_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have succesfully extracted and reviewed a very important parameter for the Logistic Regression Model. The coefficients of the model allow us to see which variables are having a larger or smaller impact on the outcome. Additionally the sign lets us know if it is a positive or negative relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Extracting a Random Forest parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will now translate the work previously undertaken on the logistic regression model to a random forest model. A parameter of this model is, for a given tree, how it decided to split at each level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This analysis is not as useful as the coefficients of logistic regression as we will be unlikely to ever explore every split and every tree in a random forest model. However, it is a very useful exercise to peak under the hood at what the model is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example we will extract a single tree from our random forest model, visualize it and programmatically extract one of the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:28.972183Z",
     "start_time": "2020-02-19T18:49:28.899380Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(max_depth=4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:30.448269Z",
     "start_time": "2020-02-19T18:49:28.976175Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:30.464227Z",
     "start_time": "2020-02-19T18:49:30.450264Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract the 7th (index 6) tree from the random forest\n",
    "chosen_tree = rf_clf.estimators_[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:30.480184Z",
     "start_time": "2020-02-19T18:49:30.467219Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the graph using Graphviz\n",
    "\n",
    "#from sklearn.tree import export_graphviz\n",
    "\n",
    "#export_graphviz(chosen_tree, out_file='assets/tree.dot', \n",
    "#                feature_names = original_variables, class_names = True,\n",
    "#                rounded = True, proportion = False, precision = 2, filled = True)\n",
    "\n",
    "# Convert to png using system command \n",
    "#from subprocess import call\n",
    "#call(['dot', '-Tpng', 'assets/tree.dot', '-o', 'assets/tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "#from IPython.display import Image\n",
    "#Image(filename = 'assets/tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:31.385452Z",
     "start_time": "2020-02-19T18:49:30.482179Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "plot_tree(chosen_tree,feature_names = original_variables, class_names = True,\n",
    "                rounded = True, proportion = False, precision = 2, filled = True, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Introducing Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the previous chapter, we learned what parameters are. We will now learn what exactly hyperparameters are, how to find and set them, as well as some tips and tricks for prioritizing our efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Hyperparameters are something that we set before the modeling process begins. We can think of them like the knobs and dials on an old radio. We tune the different dials and buttons and hope that a nice tune comes out.\n",
    "\n",
    "![Old Radio](assets/2-2.png)\n",
    "\n",
    "The algorithm does not learn the value of these during the modeling process. This is the crucial differentiator between hyperparameters and parameters. Whether we set it or the algorithm learns it and informs you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Hyperparameters in Random Forest**\n",
    "\n",
    "We can easily see the hyperparameters by creating an instance of the estimator and printing it out. Here we create the estimator with default settings and call the print function on our estimator.\n",
    "\n",
    "```\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "print(rf_clf)\n",
    "\n",
    "> RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                        criterion='gini', max_depth=None, max_features='auto',\n",
    "                        max_leaf_nodes=None, max_samples=None,\n",
    "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                        min_samples_leaf=1, min_samples_split=2,\n",
    "                        min_weight_fraction_leaf=0.0, n_estimators=10,\n",
    "                        n_jobs=None, oob_score=False, random_state=None,\n",
    "                        verbose=0, warm_start=False)\n",
    "```\n",
    "\n",
    "Those are all our different knobs and dials we can set for our model. There are a lot! But what do they all mean? For this we need to turn to the Scikit Learn documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us take the example of the `'n_estimators'` hyperparameter. We can see in the documentation that it tells us the data type and the default value. And it also provides a definition of what it means.\n",
    "\n",
    "Data Types & Default Value:\n",
    "\n",
    "```\n",
    "n_estimators: integer, optional (default=10)\n",
    "```\n",
    "\n",
    "Definition:\n",
    "\n",
    "```\n",
    "The number of trees in the forest.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Setting hyperparameters**\n",
    "\n",
    "We can set the hyperparameters when we create the estimator object. The default number of trees seems a little low, so let us set that to be 100 (***it actually 100 as default now***). Whilst we are at it, let us also set the criterion to be `'entropy'`.\n",
    "\n",
    "```\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, criterion='entropy')\n",
    "```\n",
    "\n",
    "If we print out the model We can see the other default values remain the same, but those we set explicitly overrode the default values.\n",
    "\n",
    "```\n",
    "print(rf_clf)\n",
    "\n",
    "> RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                        criterion='entropy', max_depth=None, max_features='auto',\n",
    "                        max_leaf_nodes=None, max_samples=None,\n",
    "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                        min_samples_leaf=1, min_samples_split=2,\n",
    "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                        n_jobs=None, oob_score=False, random_state=None,\n",
    "                        verbose=0, warm_start=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Hyperparameters in Logistic Regression**\n",
    "\n",
    "What about our logistic regression model, what were the hyperparameters for that? We follow the same steps. Firstly we create a logistic regression estimator. \n",
    "\n",
    "```\n",
    "log_reg_clf = LogisticRegression()\n",
    "```\n",
    "\n",
    "Then we print it out\n",
    "\n",
    "```\n",
    "print(log_reg_clf)\n",
    "\n",
    "> LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                     intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                     multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                     random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                     warm_start=False)\n",
    "```\n",
    "\n",
    "We can see there are less hyperparameters for this model than for the Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Hyperparameter Importance**\n",
    "\n",
    "Some are more important than others. But before we outline important ones, there are some hyperparameters that definitely will *not* help model performance. These are related to computational decisions or what information to retain for analysis.\n",
    "\n",
    "With the random forest classifier, these hyperparameters will not assist model performance: \n",
    "\n",
    "* `n_jobs`: how many cores to use will only speed up modeling time. \n",
    "* `random_state` & `verbose`: a random seed and whether to print out information as the modeling occurs also won't assist.\n",
    "\n",
    "Hence some hyperparameters we don't need to `'train'` during our work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Random Forest: Important Hyperparameters**\n",
    "\n",
    "There are some generally accepted important hyperparameters to tune for a Random Forest model:\n",
    "\n",
    "* `n_estimators` (how many trees in the forest) should be set to a high value, 500 or 1000 or even more is not uncommon (noting that there are computational costs to higher values).\n",
    "* `max_features` controls how many features to consider when splitting, which is vital to ensure tree diversity.\n",
    "* `max_depth` & `min_sample_leaf` control overfitting of individual trees\n",
    "* `'criterion'` hyperparameter may have a small impact but it is not generally a primary hyperparameter to consider. \n",
    "\n",
    "Remember, this is just a guide and our particular problem may require attention on other hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are hundreds of machine learning algorithms out there and learning which hyperparameters matter is knowledge we will build over time from a variety of sources. \n",
    "\n",
    "For example, there are some great academic papers where people have tried many combinations of hyperparameters for a specific algorithm on many datasets. These can be a very informative read!\n",
    "\n",
    "We can also find great blogs and tutorials online and consult the Scikit Learn documentation. Of course, one of the best ways to learn is just more practical experience! \n",
    "\n",
    "\n",
    "It is important we research this ourselves to build our knowledge base for efficient modeling. Let's explore some hyperparameters in the examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Hyperparameters in Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we saw, there are many different hyperparameters available in a Random Forest model using Scikit Learn. Let's try to differentiate between a hyperparameter and a parameter, and easily check whether something is a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can create a random forest estimator from the imported Scikit Learn package. Then print this estimator out to see the hyperparameters and their values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T09:33:20.376425Z",
     "start_time": "2020-02-17T09:33:20.366452Z"
    },
    "hidden": true
   },
   "source": [
    "Which of the following is a hyperparameter for the Scikit Learn random forest model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:31.400412Z",
     "start_time": "2020-02-19T18:49:31.387446Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Possible Answers**\n",
    "\n",
    "1. `oob_score`\n",
    "2. `classes_`\n",
    "3. `trees`\n",
    "4. `random_level`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:31.416390Z",
     "start_time": "2020-02-19T18:49:31.402409Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Pass 1,2,3 or 4 as argument\n",
    "utils.which_is_hyperparam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Exploring Random Forest Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Understanding what hyperparameters are available and the impact of different hyperparameters is a core skill for any data scientist. As models become more complex, there are many different settings we can set, but only some will have a large impact on our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will now assess an existing random forest model (it has some bad choices for hyperparameters!) and then make better choices for a new random forest model and assess its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:31.696129Z",
     "start_time": "2020-02-19T18:49:31.420359Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_clf_bad = RandomForestClassifier(n_estimators=5, random_state=42)\n",
    "rf_clf_bad.fit(X_train, y_train)\n",
    "rf_bad_predictions = rf_clf_bad.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:31.726048Z",
     "start_time": "2020-02-19T18:49:31.701140Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Get confusion matrix & accuracy for the bad rf_model\n",
    "print(f'Confusion Matrix: \\n\\n {confusion_matrix(y_test, rf_bad_predictions)} \\n Accuracy Score: \\n\\n {accuracy_score(y_test, rf_bad_predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:31.742007Z",
     "start_time": "2020-02-19T18:49:31.728043Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a new random forest classifier with better hyperparamaters\n",
    "rf_clf_new = RandomForestClassifier(n_estimators=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:57.119753Z",
     "start_time": "2020-02-19T18:49:31.744000Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fit this to the data and obtain predictions\n",
    "rf_new_predictions = rf_clf_new.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:57.135713Z",
     "start_time": "2020-02-19T18:49:57.121747Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get confusion matrix & accuracy for the new rf_model\n",
    "print(f'Confusion Matrix: \\n\\n {confusion_matrix(y_test, rf_new_predictions)} \\n Accuracy Score: \\n\\n {accuracy_score(y_test, rf_new_predictions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We got a nice 3% accuracy boost just from changing the `n_estimators`. We have had our first taste of hyperparameter tuning for a random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Hyperparameters of KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The k-nearest-neighbors algorithm is not as popular as it used to be, but can still be an excellent choice for data that has groups of data that behave similarly. Could this be the case for our credit card users?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this case we will try out several different values for one of the core hyperparameters for the knn algorithm and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:49:57.151671Z",
     "start_time": "2020-02-19T18:49:57.137707Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Build a knn estimator for each value of n_neighbours\n",
    "knn_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_10 = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_20 = KNeighborsClassifier(n_neighbors=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:50:01.810208Z",
     "start_time": "2020-02-19T18:49:57.154661Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fit each to the training data & produce predictions\n",
    "knn_5_predictions = knn_5.fit(X_train, y_train).predict(X_test)\n",
    "knn_10_predictions = knn_10.fit(X_train, y_train).predict(X_test)\n",
    "knn_20_predictions = knn_20.fit(X_train, y_train).predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:50:01.825171Z",
     "start_time": "2020-02-19T18:50:01.812203Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get an accuracy score for each of the models\n",
    "knn_5_accuracy = accuracy_score(y_test, knn_5_predictions)\n",
    "knn_10_accuracy = accuracy_score(y_test, knn_10_predictions)\n",
    "knn_20_accuracy = accuracy_score(y_test, knn_20_predictions)\n",
    "\n",
    "print(f\"The accuracy of 5, 10, 20 neighbours was {knn_5_accuracy}, {knn_10_accuracy}, {knn_20_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We succesfully tested 3 different options for 1 hyperparameter, but it was pretty exhausting. Next, we will try to find a way to make this easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Setting & Analyzing Hyperparameter Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this chapter we will look more in depth at what values to set for different hyperparameters and begin automating our work.\n",
    "\n",
    "Previously we learned that some hyperparameters are likely better to start your tuning with than others. What we didn't discuss was what values should you try. This will be specific to the algorithm and to the hyperparameter itself but there does exist best practice around this.\n",
    "\n",
    "Let's walk through some top tips for deciding ranges of values to try for different hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Conflicting Hyperparameter Choices**\n",
    "\n",
    "It is firstly important to know what values ***NOT*** to set as they may conflict.\n",
    "\n",
    "We will see in the Scikit Learn documentation for the Logistic Regression Algorithm, that some values of the hyperparameter `'penalty'` conflict with some values of the hyperparameter `'solver'`. \n",
    "\n",
    "```\n",
    "The ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties.\n",
    "```\n",
    "\n",
    "Another example from the `ElasticNet` algorithm demonstrates a softer conflict that will not result in an error, but may result in a model construction we had not anticipated.\n",
    "\n",
    "```\n",
    "This parameter is ignored when fit_intercept is set to False.\n",
    "```\n",
    "\n",
    "Safe to say, close inspection of the Scikit Learn documentation is important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Silly Hyperparamter Values**\n",
    "\n",
    "There are also values for different hyperparameters that may be valid but are very unlikely to yield good results. Some examples of this are:\n",
    "\n",
    "* Having a random forest algorithm with a very low number of trees. Would we consider it a forest if it had 2 trees? How about 5 or 10? Still probably not. But at 300, 500, 1000 or more that is definitely getting there!\n",
    "* Having only 1 neighbor in a K-nearest neighbor algorithm. This algorithm averages votes of `'neighbors'` to our sample. Safe to say averaging the vote of 1 person doesn't sound robust!\n",
    "* Finally, incrementing some hyperparameters by a small amount is unlikely to greatly improve the model. One more tree in a forest for example, isn't likely to have a large impact. \n",
    "\n",
    "Researching and documenting sensible values for different hyperparameters and algorithms will be a very useful activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In a previous example we built several different models to test a single hyperparameter like so. \n",
    "\n",
    "```\n",
    "knn_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_10 = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_20 = KNeighborsClassifier(n_neighbors=20)\n",
    "```\n",
    "\n",
    "This was quite an inefficient way of writing code, and we can do better to test different values for the number of neighbors hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Automating Hyperparameter Tuning**\n",
    "\n",
    "One thing we could try is using a for loop. We create a list of values to test. Then loop through the list, creating an estimator, fitting and predicting each time. We append the accuracy to a list of accuracy scores to analyze after. \n",
    "\n",
    "```\n",
    "neighbors_list = [3,5,10,20,50,75]\n",
    "\n",
    "for test_number in neighbors_list:\n",
    "    model = KNeighborsClassifier(n_neighbors=test_number)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy_list.append(accuracy)\n",
    "```\n",
    "\n",
    "This method easily allows us to test more values than our previous work. We can store the results in a DataFrame to view the effect of this hyperparameter on the accuracy of the model.\n",
    "\n",
    "```\n",
    "results_df = pf.DataFrame({'neighbors': neighbors_list, 'accuracy': accuracy_list})\n",
    "print(results_df)\n",
    "```\n",
    "\n",
    "| Neighbors | Accuracy  |\n",
    "| --------: | --------: |\n",
    "| 3         | 0.71      |\n",
    "| 5         | 0.7125    |\n",
    "| 10        | 0.765     |\n",
    "| 20        | 0.7825    |\n",
    "| 50        | 0.7825    |\n",
    "| 75        | 0.7825    |\n",
    "\n",
    "It appears that adding any more neighbors doesn't help beyond 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Learning Curves**\n",
    "\n",
    "A common tool that is used to assist with analyzing the impact of a singular hyperparameter on an end result is called a `'learning curve'`.\n",
    "\n",
    "Firstly let's create a list of many more values to test using Python's range function. The rest of the code is the same\n",
    "as before. \n",
    "\n",
    "```\n",
    "neighbors_list = list(range(5,500,5))\n",
    "\n",
    "for test_number in neighbors_list:\n",
    "    model = KNeighborsClassifier(n_neighbors=test_number)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "results_df = pf.DataFrame({'neighbors': neighbors_list, 'accuracy': accuracy_list})   \n",
    "```\n",
    "\n",
    "Since we tested so many values, we will use a graph rather than a table to analyze the results. We plot the accuracy score on the Y axis and our hyperparameter value on our X axis. \n",
    "\n",
    "```\n",
    "plt.plot(results_df['neighbors'], results_df['accuracy'])\n",
    "\n",
    "# Add the labels and title\n",
    "plt.gca().set(xlabel='n_neighbors', ylabel='Accuracy', title='Accuracy for different n_neighbors')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![Learning curves](assets/2-3.png)\n",
    "\n",
    "We can see our suspicions confirmed, that accuracy does not increase at all beyond where we tested before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One thing to be aware of is that python's range function does not work for decimal steps which is important for hyperparameters that work on that scale. A handy trick uses NumPy's `linspace` function that will create a number of values, evenly spread between a start and end value that we specify.\n",
    "\n",
    "Here is a quick example, 5 values, evenly spaced between 1 and 2 inclusive:\n",
    "\n",
    "```\n",
    "print(np.linspace(1,2,5))\n",
    "\n",
    "> [1. 1.25 1.5 1.75 2.]\n",
    "```\n",
    "\n",
    "Let's practice trying different hyperparameters and plotting some learning curves!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Automating Hyperparameter Choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finding the best hyperparameter of interest without writing hundreds of lines of code for hundreds of models is an important efficiency gain that will greatly assist our future machine learning model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An important hyperparameter for the GBM algorithm is the learning rate. But which learning rate is best for this problem? By writing a loop to search through a number of possibilities, collating these and viewing them we can find the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Possible learning rates to try include 0.001, 0.01, 0.05, 0.1, 0.2 and 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:50:01.840130Z",
     "start_time": "2020-02-19T18:50:01.829157Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:50:01.855089Z",
     "start_time": "2020-02-19T18:50:01.842125Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set the learning rates & results storage\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "results_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:50:43.360578Z",
     "start_time": "2020-02-19T18:50:01.858082Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create the for loop to evaluate model predictions for each learning rate\n",
    "for learning_rate in learning_rates:\n",
    "    model = GradientBoostingClassifier(learning_rate=learning_rate, random_state=42)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    # Save the learning rate and accuracy score\n",
    "    results_list.append([learning_rate, accuracy_score(y_test, predictions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:50:43.375538Z",
     "start_time": "2020-02-19T18:50:43.362573Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Gather everything into a DataFrame\n",
    "results_df = pd.DataFrame(results_list, columns=['learning_rate', 'accuracy'])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We efficiently tested a few different values for a single hyperparameter and can easily see which learning rate value was the best. Here, it seems that a learning rate of 0.05 yields the best accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Building Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we want to test many different values for a single hyperparameter it can be difficult to easily view that in the form of a DataFrame. Previously we learned about a nice trick to analyze this. A graph called a 'learning curve' can nicely demonstrate the effect of increasing or decreasing a particular hyperparameter on the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Instead of testing only a few values for the learning rate, we will test many to easily see the effect of this hyperparameter across a large range of values. A useful function from NumPy is `np.linspace(start, end, num)` which allows us to create a number of values (`num`) evenly spread within an interval (`start`, `end`) that we specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:50:43.391495Z",
     "start_time": "2020-02-19T18:50:43.379528Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Set the learning rates & accuracies list\n",
    "learn_rates = np.linspace(0.01, 2, num=30)\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:54:05.080909Z",
     "start_time": "2020-02-19T18:50:43.394487Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create the for loop\n",
    "for learn_rate in learn_rates:\n",
    "    # Create the model, predictions & save the accuracies as before\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate, random_state=42)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:54:05.328196Z",
     "start_time": "2020-02-19T18:54:05.082850Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot results    \n",
    "plt.plot(learn_rates, accuracies)\n",
    "plt.gca().set(xlabel='learning_rate', ylabel='Accuracy', title='Accuracy for different learning_rates')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can see that for low values, we get a pretty good accuracy. However once the learning rate pushes much above 1.5, the accuracy starts to drop. We have learned and practiced a useful skill for visualizing large amounts of results for a single hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This part of the lesson introduces a popular automated hyperparameter tuning methodology called Grid Search. We will learn what it is, how it works and practice undertaking a Grid Search using Scikit Learn. We will then learn how to analyze the output of a Grid Search & gain practical experience doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Introducing Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this section we will look at extending our work on automatic hyperparameter tuning and learn what a Grid Search is.\n",
    "\n",
    "Let's remind ourselves of your previous work using a for loop to test different values of the number of neighbors in a KNN algorithm. We then collated those into a DataFrame to analyze. \n",
    "\n",
    "But what if we want to test different values of 2 hyperparameters? Let us take the example of a GBM algorithm, which has a few more hyperparameters to tune than KNN or Random Forest algorithms. Let's' say we want to tune the two hyperparameters and values as follows:\n",
    "\n",
    "* `learn_rate` - [0.001, 0.01, 0.05]\n",
    "* `max_depth` - [4,6,8,10]\n",
    "\n",
    "How would you do that? One suggestion could be a nested loop. With a nested loop we test all values of our first hyperparameter for all values of our second hyperparameter.\n",
    "\n",
    "**How many models**\n",
    "\n",
    "We will notice that many more models are built when adding more hyperparameters and values to test. Importantly, this relationship between models created and hyperparameters or values to test is not a linear relationship, it is exponential. For each of the values tested for the first hyperparameter, we test every value of the second hyperparameter.\n",
    "\n",
    "This means to test 5 values for the first hyperparameter and 10 values for the second hyperparameter, we have 50 models to run. And what if we k-fold cross-validated each model 10 times? That would be 500 models to run! That was just for 2 hyperparameters. What if we wanted to test a third or fourth hyperparameter? We could nest again (and again). \n",
    "\n",
    "Safe to say we cannot keep nesting forever as our code becomes complex and inefficient. Plus, what if we also wanted some extra information on training and testing times and scores. Our code will get quite complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's review our work in an alternate way. If we created a grid with each value of `max_depth` that we want to test down the left and each value of `learning_rate` across the top. The intersection square of each of these is a model that we need to run. \n",
    "\n",
    "![Grid Search](assets/2-12.png)\n",
    "\n",
    "\n",
    "Running a model for every cell in the grid with the hyperparameters specified is known as a `Grid Search`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Grid search has a number of advantages:\n",
    "\n",
    "* It's programmatic, and saves many lines of code.\n",
    "* It is guaranteed to find the best model within the grid we specify. But if we specify a poor grid with silly or\n",
    "conflicting values we won't get a good score!\n",
    "* Finally, it is an easy methodology to explain\n",
    "\n",
    "However there are some disadvantages to this approach: \n",
    "* It is computationally expensive. \n",
    "* It is also 'uninformed' because it doesn't learn as it creates models the next model it creates could be better or worse.\n",
    "\n",
    "Let's now practice undertaking a grid search!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Build Grid Search functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In data science it is a great idea to try building algorithms, models and processes 'from scratch' so we can really understand what is happening at a deeper level. Of course there are great packages and libraries for this work (and we will get to that very soon!) but building from scratch will give us a great edge in our data science work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example, we will create a function to take in 2 hyperparameters, build models and return results. We will use this function in a future example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:54:05.343154Z",
     "start_time": "2020-02-19T18:54:05.330189Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create the function\n",
    "def gbm_grid_search(learn_rate, max_depth, random_state=42):\n",
    "\n",
    "    # Create the model\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth, random_state=random_state)\n",
    "    \n",
    "    # Use the model to make predictions\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Return the hyperparameters and score\n",
    "    return([learn_rate, max_depth, accuracy_score(y_test, predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Iteratively tune multiple hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example, we will build on the function we previously created to take in 2 hyperparameters, build a model and return the results. We will now use that to loop through some values and then extend this function and loop with another hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:54:05.358113Z",
     "start_time": "2020-02-19T18:54:05.344151Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create the relevant lists\n",
    "results_list = []\n",
    "learn_rate_list = [0.01, 0.1, 0.5]\n",
    "max_depth_list = [2,4,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:55:25.042572Z",
     "start_time": "2020-02-19T18:54:05.360108Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create the for loop\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        results_list.append(gbm_grid_search(learn_rate,max_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:55:25.058530Z",
     "start_time": "2020-02-19T18:55:25.044567Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Print the results\n",
    "results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:55:25.074519Z",
     "start_time": "2020-02-19T18:55:25.062520Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extend the function input\n",
    "def gbm_grid_search_extended(learn_rate, max_depth, subsample, random_state=42):\n",
    "\n",
    "    # Extend the model creation section\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth, subsample=subsample, random_state=42)\n",
    "    \n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Extend the return part\n",
    "    return([learn_rate, max_depth, subsample, accuracy_score(y_test, predictions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:55:25.090444Z",
     "start_time": "2020-02-19T18:55:25.078477Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "# Create the new list to test\n",
    "subsample_list = [0.4, 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:56:51.075867Z",
     "start_time": "2020-02-19T18:55:25.092467Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        # Extend the for loop\n",
    "        for subsample in subsample_list:\n",
    "            # Extend the results to include the new hyperparameter\n",
    "            results_list.append(gbm_grid_search_extended(learn_rate, max_depth, subsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:56:51.091085Z",
     "start_time": "2020-02-19T18:56:51.078159Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Print results\n",
    "results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have effectively built our own grid search! We went from 2 to 3 hyperparameters and can see how we could extend that to even more values and hyperparameters. That was a lot of effort though. Be warned - we are now entering a world that can get very computationally expensive very fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### How Many Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Adding more hyperparameters or values, we increase the amount of models created but the increase is not linear, it is proportional to how many values and hyperparameters we already have.\n",
    "\n",
    "How many models would be created when running a grid search over the following hyperparameters and values for a GBM algorithm?\n",
    "\n",
    "* learning_rate = [0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 1, 2]\n",
    "* max_depth = [4,6,8,10,12,14,16,18, 20]\n",
    "* subsample = [0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "* max_features = ['auto', 'sqrt', 'log2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Possible Answers**\n",
    "\n",
    "1. 26\n",
    "2. 9 of one model, 9 of another\n",
    "3. 1 large model\n",
    "4. 1215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:56:51.107041Z",
     "start_time": "2020-02-19T18:56:51.094075Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Enter 1, 2, 3 or 4 as the answer\n",
    "utils.how_many_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Grid Search with Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this chapter we will move beyond our manual code and leverage Scikit Learn's `GridSearchCV` to assist our grid search. It will help us create a grid search more efficiently and get some performance analytics.\n",
    "\n",
    "This is an example of a GridSearchCV object: \n",
    "\n",
    "```\n",
    "sklearn.model_selection.GridSearchCV(\n",
    "    estimator,\n",
    "    param_grid, scoring=None, fit_params=None,\n",
    "    n_jobs=None, iid='warm', refit=True, cv='warn,\n",
    "    verbose=0, pre_dispatch='2*n_jobs',\n",
    "    error_score='raise-deprecating',\n",
    "    return_train_score='warn')\n",
    "```\n",
    "\n",
    "Don't worry, we will break it down!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Steps in a Grid Search**\n",
    "\n",
    "Firstly, let us conceptualize the steps needed to do a proper grid search. Some of these will be familiar from our manual work before:\n",
    "\n",
    "1. Select an algorithm (or 'estimator') to tune\n",
    "2. Define which hyperparameters we will tune\n",
    "3. Define a range of values for each hyperparameter\n",
    "4. Decide a cross-validation scheme\n",
    "5. Define a scoring function to determine which model was the best\n",
    "6. Include extra useful information or functions.\n",
    "\n",
    "The only one of these we did not do much work with previously is step (4), but we will cover each now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**GridSearchCV Object Inputs**\n",
    "\n",
    "A GridSearchCV object takes several important arguments:\n",
    "\n",
    "* estimator\n",
    "* param_grid\n",
    "* cv\n",
    "* scoring\n",
    "* refit\n",
    "* n_jobs\n",
    "* return_train_score\n",
    "\n",
    "The `estimator` is our algorithm. Examples include KNN, Random Forest, GBM or Logistic Regression. We only pick one algorithm for each grid search.\n",
    "\n",
    "`param_grid` is how we tell GridSearchCV which hyperparameters and which values to test. We were previously using lists, but param_grid needs a dictionary. The dictionary keys must be the hyperparameter names, the values a list of values to test. The keys in the param_grid dictionary must be valid hyperparameters else the Grid Search will fail.\n",
    "\n",
    "```\n",
    "param_grid = { 'max_depth': [2, 4, 6, 8],\n",
    "               'min_samples_leaf': [1, 2, 4, 6]}\n",
    "```\n",
    "\n",
    "The `cv` input allows us to undertake cross-validation. We could specify different cross-validation types here. But simply providing an integer will create a k-fold. We are likely familiar with standard 5 and 10 k-fold cross validation.\n",
    "\n",
    "![k-fold](assets/2-4.png)\n",
    "\n",
    "`scoring` is a scoring function used to evaluate our model's performance. We did this manually previously using accuracy. We can use our own custom metric, or one from the available metrics from Scikit Learn's `metrics` module. We can check all available metrics using this command:\n",
    "\n",
    "```\n",
    "from sklearn import metrics\n",
    "sorted(metris.SCORERS.keys())\n",
    "```\n",
    "\n",
    "`refit` set to true means the best hyperparameter combinations are used to undertake a fitting to the training data. The GridSearchCV object can be used as an estimator directly. This is very handy as we don't need to save our the best hyperparameters and train another model.\n",
    "\n",
    "`n_jobs` assists with parallel execution. We can effectively 'split up' your work and have many models being created at the same time. This is possible because the results of one model do not affect the next one. We can check how many cores we have available, which determines how many models we can run in parallel using this handy code:\n",
    "\n",
    "```\n",
    "import os\n",
    "print(os.cpu_count())\n",
    "```\n",
    "\n",
    "We must be careful using all cores for a task though as this may mean we can't do other work on our computer while your models run.\n",
    "\n",
    "Finally `return_train_score` logs statistics about the training runs that were undertaken. This can be useful for plotting and understanding test vs training set performance (and hence bias-variance tradeoff). While informative, this is computationally expensive and will not assist in finding the best model.\n",
    "\n",
    "\n",
    "Now we have all the components to build a grid search object!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### GridSearchCV inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's test our knowledge of GridSeachCV inputs by answering the question below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Three GridSearchCV objects are available below. Note that there is no data available to fit these models. Instead, answer by looking at their construct.\n",
    "\n",
    "```\n",
    "Model #1:\n",
    " GridSearchCV(cv=5, error_score='raise-deprecating',\n",
    "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False),\n",
    "       fit_params=None, iid='warn', n_jobs=4,\n",
    "       param_grid={'max_depth': [2, 4, 8, 15], 'max_features': ['auto', 'sqrt']},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
    "       scoring='roc_auc', verbose=0) \n",
    "\n",
    "\n",
    "Model #2:\n",
    " GridSearchCV(cv=10, error_score='raise-deprecating',\n",
    "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
    "           weights='uniform'),\n",
    "       fit_params=None, iid='warn', n_jobs=8,\n",
    "       param_grid={'n_neighbors': [5, 10, 20], 'algorithm': ['ball_tree', 'brute']},\n",
    "       pre_dispatch='2*n_jobs', refit=False, return_train_score='warn',\n",
    "       scoring='accuracy', verbose=0) \n",
    "\n",
    "\n",
    "Model #3:\n",
    " GridSearchCV(cv=7, error_score='raise-deprecating',\n",
    "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
    "              max_features=None, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=1, min_sampl...      subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
    "              verbose=0, warm_start=False),\n",
    "       fit_params=None, iid='warn', n_jobs=2,\n",
    "       param_grid={'number_attempts': [2, 4, 6], 'max_depth': [3, 6, 9, 12]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring='accuracy', verbose=0)\n",
    "```\n",
    "\n",
    "Which of these GridSearchCV objects would not work when we try to fit it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Possible Answers**\n",
    "\n",
    "1. `Model #1` would not work when we try to fit it.\n",
    "2. `Model #2` would not work when we try to fit it.\n",
    "3. `Model #3` would not work when we try to fit it.\n",
    "4. None - they will all work when we try to fit them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:56:51.122001Z",
     "start_time": "2020-02-19T18:56:51.111028Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Enter 1, 2, 3 or 4 as the answer\n",
    "utils.which_grid_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### GridSearchCV with Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `GridSearchCV` module from Scikit Learn provides many useful features to assist with efficiently undertaking a grid search. We will now put it into practice by creating a GridSearchCV object with certain parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The desired options are:\n",
    "\n",
    "* A Random Forest Estimator, with the split criterion as 'entropy'\n",
    "* 5-fold cross validation\n",
    "* The hyperparameters `max_depth` (2, 4, 8, 15) and `max_features` ('auto' vs 'sqrt')\n",
    "* Use `roc_auc` to score the models\n",
    "* Use 4 cores for processing in parallel\n",
    "* Ensure we refit the best model and return training scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:56:51.137957Z",
     "start_time": "2020-02-19T18:56:51.123995Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a Random Forest Classifier with specified criterion\n",
    "rf_class = RandomForestClassifier(criterion='entropy', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:56:51.153913Z",
     "start_time": "2020-02-19T18:56:51.139953Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "param_grid = {'max_depth': [2, 4, 8, 15], 'max_features': ['auto', 'sqrt']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:56:51.169873Z",
     "start_time": "2020-02-19T18:56:51.156919Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a GridSearchCV object\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_rf_class=GridSearchCV(\n",
    "    estimator=rf_class,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=4,\n",
    "    cv=5,\n",
    "    refit=True, \n",
    "    return_train_score=True)\n",
    "\n",
    "grid_rf_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Understanding a grid search output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we know how to run a grid search, let's focus on its output. Let us now analyze each of the properties of the GridSearchCV output and learn how to access and use them. The properties of the object can be categorized into three different groups:\n",
    "\n",
    "* a results log: `cv_results_`\n",
    "* the best results: `best_index_`, `best_params_` & `best_index_`\n",
    "* 'Extra information': `scorer_`, `n_splits_` & `refit_time_`\n",
    "\n",
    "Properties are accessed using the dot notation, that is `grid_search_object.property`. Where `property` is the actual property you want to retrieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's review each of the key properties now:\n",
    "\n",
    "Firstly there is the `cv_results_` property. This is a dictionary that we can read into a pandas DataFrame to explore.\n",
    "\n",
    "The `param_` columns contain information on the different parameters that were used in the model. \n",
    "\n",
    "![CV Results](assets/2-14.png)\n",
    "\n",
    "Remember, each row in this DataFrame is about one model. So we can see row 3 for example tested the hyperparameter combination of `max_depth=10` ,`min_samples_leaf=2` nd `n_estimators=100` for our random forest estimator. \n",
    "\n",
    "The `params` column is a dictionary of all the parameters from the previous `'param'` columns.\n",
    "\n",
    "![Params](assets/2-15.png)\n",
    "\n",
    "We need to use `pd.set_option` here to ensure we don't truncate the results we are printing:\n",
    "\n",
    "```\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(cv_results_df.loc[:, 'params'])\n",
    "```\n",
    "\n",
    "The next 5 columns are the testing scores for each of the 5 cross-folds, or splits, we made, followed by the the mean and standard deviation for those cross-folds. \n",
    "\n",
    "![CV Results](assets/2-13.png)\n",
    "\n",
    "The rank column conveniently ranks the rows by the `mean_test_score`.\n",
    "\n",
    "![Params](assets/2-16.png)\n",
    "\n",
    "We can see that the model in our third row had the best mean_test_score.\n",
    "\n",
    "Using the `rank_test_score` column we can easily select the grid search square for analysis. \n",
    "\n",
    "\n",
    "```\n",
    "best_row = cv_results_df[cv_results_df['rank_test_score'] == 1]\n",
    "prnt(best_row)\n",
    "```\n",
    "\n",
    "![Params](assets/2-17.png)\n",
    "\n",
    "This table is the row from the `cv_results` object that was the best model created. \n",
    "\n",
    "The `test_score` columns are then repeated for the training scores. Note that if we had not set `return_train_score` to True this would not include the training scores. There is also no ranking column for the training scores, as we only care about performance on the test set in each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Information on the best grid square is found in three different properties:\n",
    "\n",
    "* `best_params_` which is the dictionary of the parameters that gave the best score. \n",
    "* `best_score_`, the actual best score \n",
    "* `best_index`, the row in our `cv_results_` that was the best. This is same as the index of the row with rank 1 in cv_results_ that we extracted just before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "GridSearchCV stores an estimator built with the best hyperparameters in the `best_estimator_` property. Since it is an estimator, we can use this to predict on our test set.\n",
    "\n",
    "We can also use the GridSearchCV object itself directly as an estimator. This is why we set refit=True when creating the grid search, otherwise we would need to refit using the best parameters ourself before using the best estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Extra information**\n",
    "\n",
    "Some extra information can be obtained with the following properties. These are not very useful properties but may be important if you construct you grid search differently\n",
    "\n",
    "These include the scorer function that was used and the number of cross validation splits (both of which we set ourselves), and the refit_time which is the number of seconds used for refitting the best model on the whole dataset. \n",
    "\n",
    "This may be of interest in analyzing efficiencies in our work, but not for our use case here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Exploring the grid search results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will now explore the `cv_results_` property of the GridSearchCV object defined above. This is a dictionary that we can read into a pandas DataFrame and contains a lot of useful information about the grid search we just undertook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A reminder of the different column types in this property:\n",
    "\n",
    "* `time_` columns\n",
    "* `param_` columns (one for each hyperparameter) and **the** singular `params` column (with all hyperparameter settings)\n",
    "* a `train_score` column for each cv fold including the `mean_train_score` and `std_train_score` columns\n",
    "* a `test_score` column for each cv fold including the `mean_test_score` and `std_test_score` columns\n",
    "* a `rank_test_score` column with a number from 1 to n (number of iterations) ranking the rows based on their `mean_test_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:24.514811Z",
     "start_time": "2020-02-19T18:56:51.172862Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# First fit the GridSearchCV\n",
    "grid_rf_class.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:24.560682Z",
     "start_time": "2020-02-19T18:57:24.516773Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read the cv_results property into a dataframe & print it out\n",
    "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
    "cv_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:24.591606Z",
     "start_time": "2020-02-19T18:57:24.564647Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract and print the column with a dictionary of hyperparameters used\n",
    "column = cv_results_df.loc[:, [\"params\"]]\n",
    "column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:24.623487Z",
     "start_time": "2020-02-19T18:57:24.593569Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Extract and print the row that had the best mean test score\n",
    "best_row = cv_results_df[cv_results_df[\"rank_test_score\"] == 1]\n",
    "best_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Analyzing the best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At the end of the day, we primarily care about the best performing 'square' in a grid search. Luckily Scikit Learn's `gridSearchCV` objects have a number of parameters that provide key information on just the best square (or row in `cv_results_`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Three properties we will explore are:\n",
    "\n",
    "* `best_score_` – The score (here ROC_AUC) from the best-performing square.\n",
    "* `best_index_` – The index of the row in `cv_results_` containing information on the best-performing square.\n",
    "* `best_params_` – A dictionary of the parameters that gave the best score, for example `'max_depth': 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:24.638448Z",
     "start_time": "2020-02-19T18:57:24.625504Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Print out the ROC_AUC score from the best-performing square\n",
    "best_score = grid_rf_class.best_score_\n",
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:24.668368Z",
     "start_time": "2020-02-19T18:57:24.641452Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a variable from the row related to the best-performing square\n",
    "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
    "best_row = cv_results_df.loc[[grid_rf_class.best_index_]]\n",
    "best_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:24.683353Z",
     "start_time": "2020-02-19T18:57:24.670384Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get the max_depth parameter from the best-performing square\n",
    "best_max_depth = grid_rf_class.best_params_[\"max_depth\"]\n",
    "best_max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Being able to quickly find and prioritize the huge volume of information given back from machine learning modeling output is a great skill. Here we had great practice doing that with `cv_results_` by quickly isolating the key information on the best performing square. This will be very important when our grids grow from 12 squares to many more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Using the best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "While it is interesting to analyze the results of our grid search, our final goal is practical in nature; we want to make predictions on our test set using our estimator object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can access this object through the `best_estimator_` property of our grid search object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example, we will take a look inside the `best_estimator_` property and then use this to make predictions on our test set for credit card defaults and generate a variety of scores. Remember that we need to use `predict_proba` rather than `predict` since we need probability values rather than class labels for our roc_auc score. We use a slice `[:,1]` to get probabilities of the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:24.698289Z",
     "start_time": "2020-02-19T18:57:24.686352Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# See what type of object the best_estimator_ property is\n",
    "type(grid_rf_class.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:24.808031Z",
     "start_time": "2020-02-19T18:57:24.700283Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create an array of predictions directly using the best_estimator_ property\n",
    "predictions = grid_rf_class.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:24.822979Z",
     "start_time": "2020-02-19T18:57:24.809989Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Take a look to confirm it worked, this should be an array of 1's and 0's\n",
    "predictions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:24.838913Z",
     "start_time": "2020-02-19T18:57:24.824950Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now create a confusion matrix \n",
    "print(\"Confusion Matrix \\n\", confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:24.948661Z",
     "start_time": "2020-02-19T18:57:24.839944Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Get the ROC-AUC score\n",
    "predictions_proba = grid_rf_class.best_estimator_.predict_proba(X_test)[:,1]\n",
    "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test, predictions_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `.best_estimator_` property is a really powerful property to understand for streamlining our machine learning model building process. We now can run a grid search and seamlessly use the best model from that search to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular automated hyperparameter tuning methodology is called Random Search. We will learn what it is, how it works and importantly how it differs from grid search. We will learn some advantages and disadvantages of this method and when to choose this method compared to Grid Search. We will practice undertaking a Random Search with Scikit Learn as well as visualizing & interpreting the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introducing Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this chapter we will cover the concept of random search, how it differs from grid search, as well as some advantages and disadvantages of this method.\n",
    "\n",
    "A random search is very similar to a grid search several key steps. We still define an estimator, which hyperparameters to tune and the range of values for each hyperparameter. Similarly, we still set a cross-validation scheme and scoring function. However when it comes to undertaking the search, rather than trying every single combination, we randomly sample N combinations and try these out. \n",
    "\n",
    "This may seem like a really odd thing to do. Why would we do this and why does it work?\n",
    "\n",
    "An important paper by Yoshua Bengio and James Bergstra outlines that reason that this works can be explained with two fundamental principles:\n",
    "\n",
    "* Not every hyperparameter is as important (we hinted at this in the Grid Search Section)\n",
    "* A little trick of probability.\n",
    "\n",
    "Let's explain the probability trick.\n",
    "\n",
    "Let's say that we have this grid. It has 100 cells, so 100 different models. 10 different values each of two hyperparameters. Let us say that these 5 models are the best, highlighted in green. \n",
    "\n",
    "\n",
    "![Random Search](assets/2-5.png)\n",
    "\n",
    "How many models would we need to run with random search to have a 95% chance of getting one of the green squares?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's consider how likely it is that we continue to completely miss the good models, if we randomly select hyperparameter combinations uniformly:\n",
    "\n",
    "* On our first trial we have 5% chance of getting one of these squares as it is 5 squares out of 100. Therefore we have `(1-0.05)` chance of missing these squares. \n",
    "* If we do a second trial, we now have `(1-0.05) * (1-0.05)` of missing that range. \n",
    "* For a third trial we have `(1-0.05) * (1-0.05) * (1-0.05)` chance of missing that range. \n",
    "\n",
    "In fact, with n trials we have (1-0.05)^n chance that every single trial misses all the good models. So how many trials to have a high chance of being in the region?\n",
    "\n",
    "We know that the probability of missing everything is `(1-0.05)^n`. So the probability of getting something in that area must be `1-(miss everything)` which is `1-(1-0.05)^n`. Without going into too much details, we can solve to get the answer as `n >= 59`. So what does that all mean?\n",
    "\n",
    "With relatively few trials we can get close to our maximum score with a relatively high probability. In essence, it is very unlikely that we will continue to miss everything for a long time. A Grid Search may spend lots of time covering a bad area. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Some important notes**\n",
    "\n",
    "There are some important things to keep in mind with random search. Our possible score is still only as good as the grid we set! If we a bad grid to sample from, we will not get a great model. \n",
    "\n",
    "Remember to fairly compare this to grid search, we need to have the same modeling 'budget'. For example, 200 models on grid search and 200 models in random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Creating a random sample of hyperparameters**\n",
    "\n",
    "We can create our own random sample of hyperparameter combinations. We firstly set the hyperparameter lists as we have done before with numpy linspace and a range function. \n",
    "\n",
    "```\n",
    "# Set some hyperparameter lists\n",
    "learn_rate_list = np.linspace(0.001, 2, 150)\n",
    "min_samples_leaf_list = list(range(1, 51))\n",
    "```\n",
    "\n",
    "We can then create a single list of hyperparameter combinations that we can sample from using itertools `product` function. \n",
    "\n",
    "```\n",
    "# Create list of combinations\n",
    "from itertools import product\n",
    "combinations_list = [list(x) for x in product(learn_rate_list, min_samples_leaf_list)]\n",
    "```\n",
    "\n",
    "Then we can randomly select 100 models from these lists using NumPy's handy `random.choice` function, which gives us 100 random\n",
    "indexes we can use to index into the created list in the last line.\n",
    "\n",
    "```\n",
    "# Select 100 models from our larger set\n",
    "random_combinations_index = np.random.choice(range(0, len(combinations_list)), 100, replace=False)\n",
    "combinations_random_chosen = [combinations_list[x] for x in random_combinations_index]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Visualizing a Random Search**\n",
    "\n",
    "A visual inspection of the hyperparameter values chosen by random search is a nice way to demonstrate how it works. \n",
    "\n",
    "![Random Search Graph](assets/2-6.png)\n",
    "\n",
    "Notice how the coverage of this is very wide but it does not cover thoroughly? Let's practice sampling and visualizing our own random search!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Randomly Sample Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To undertake a random search, we firstly need to undertake a random sampling of our hyperparameter space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example, we will firstly create some lists of hyperparameters that can be zipped up to a list of lists. Then we will randomly sample hyperparameter combinations preparation for running a random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will use just the hyperparameters `learning_rate` and `min_samples_leaf` of the GBM algorithm to keep the example illustrative and not overly complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:24.964575Z",
     "start_time": "2020-02-19T18:57:24.950612Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a list of values for the learning_rate hyperparameter\n",
    "learn_rate_list = list(np.linspace(0.01,1.5,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:24.980533Z",
     "start_time": "2020-02-19T18:57:24.966575Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a list of values for the min_samples_leaf hyperparameter\n",
    "min_samples_list = list(range(10,41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:24.996523Z",
     "start_time": "2020-02-19T18:57:24.982529Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Combination list\n",
    "from itertools import product\n",
    "combinations_list = [list(x) for x in product(learn_rate_list, min_samples_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:25.012448Z",
     "start_time": "2020-02-19T18:57:24.998485Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sample hyperparameter combinations for a random search.\n",
    "random_combinations_index = np.random.choice(range(0, len(combinations_list)), 250, replace=False)\n",
    "combinations_random_chosen = [combinations_list[x] for x in random_combinations_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:25.043365Z",
     "start_time": "2020-02-19T18:57:25.014443Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Print the result\n",
    "combinations_random_chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We generated some hyperparameter combinations and randomly sampled in that space. The output was not too nice though, in the next example we will use a much more efficient method for this. In a future example we will also make this output look much nicer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Randomly Search with Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To solidify our knowledge of random sampling, let's try a similar exercise but using different hyperparameters and a different algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As before, we create some lists of hyperparameters that can be zipped up to a list of lists. We will use the hyperparameters `criterion`, `max_depth` and `max_features` of the random forest algorithm. Then we will randomly sample hyperparameter combinations in preparation for running a random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will use a slightly different package for sampling in this task, `random.sample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:25.059347Z",
     "start_time": "2020-02-19T18:57:25.051367Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create lists for criterion and max_features\n",
    "criterion_list = [\"gini\", \"entropy\"]\n",
    "max_feature_list = [\"auto\", \"sqrt\", \"log2\", None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:25.074281Z",
     "start_time": "2020-02-19T18:57:25.064308Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a list of values for the max_depth hyperparameter\n",
    "max_depth_list = list(range(3,56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:25.089274Z",
     "start_time": "2020-02-19T18:57:25.075279Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Combination list\n",
    "combinations_list = [list(x) for x in product(criterion_list, max_feature_list, max_depth_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:25.104202Z",
     "start_time": "2020-02-19T18:57:25.091237Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Sample hyperparameter combinations for a random search\n",
    "combinations_random_chosen = random.sample(combinations_list, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:25.119171Z",
     "start_time": "2020-02-19T18:57:25.106198Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Print the result\n",
    "combinations_random_chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Visualizing a Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Visualizing the search space of random search allows us to easily see the coverage of this technique and therefore allows us to see the effect of our sampling on the search space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example we will use several different samples of hyperparameter combinations and produce visualizations of the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:25.135118Z",
     "start_time": "2020-02-19T18:57:25.121157Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sample_hyperparameters(n_samples):\n",
    "    global combinations_random_chosen\n",
    "\n",
    "    if n_samples == len(combinations_list):\n",
    "        combinations_random_chosen = combinations_list\n",
    "        return\n",
    "\n",
    "    combinations_random_chosen = []\n",
    "    random_combinations_index = np.random.choice(range(0, len(combinations_list)), n_samples, replace=False)\n",
    "    combinations_random_chosen = [combinations_list[x] for x in random_combinations_index]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:25.151104Z",
     "start_time": "2020-02-19T18:57:25.138133Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualize_search():\n",
    "    rand_y, rand_x = [x[0] for x in combinations_random_chosen], [x[1] for x in combinations_random_chosen]\n",
    "\n",
    "    # Plot all together\n",
    "    plt.clf() \n",
    "    plt.scatter(rand_y, rand_x, c=['blue']*len(combinations_random_chosen))\n",
    "    plt.gca().set(xlabel='learn_rate', ylabel='min_samples_leaf', title='Random Search Hyperparameters')\n",
    "    plt.gca().set_xlim([0.01, 1.5])\n",
    "    plt.gca().set_ylim([10, 29])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:25.166044Z",
     "start_time": "2020-02-19T18:57:25.154070Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a list of values for the learning_rate hyperparameter\n",
    "learn_rate_list = list(np.linspace(0.01,1.5,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:25.181995Z",
     "start_time": "2020-02-19T18:57:25.169028Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a list of values for the min_samples_leaf hyperparameter\n",
    "min_samples_list = list(range(10,41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:25.197021Z",
     "start_time": "2020-02-19T18:57:25.183989Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Combination list\n",
    "combinations_list = [list(x) for x in product(learn_rate_list, min_samples_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:25.212910Z",
     "start_time": "2020-02-19T18:57:25.198950Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Confirm how hyperparameter combinations & print\n",
    "number_combs = len(combinations_list)\n",
    "number_combs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:26.235744Z",
     "start_time": "2020-02-19T18:57:25.214905Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sample and visualise combinations\n",
    "for x in [250, 1500, 5000]:\n",
    "    sample_hyperparameters(x)\n",
    "    visualize_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Notice how the bigger our sample space of a random search the more it looks like a grid search? In a later example we will look closer at comparing these two methods side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Random Search in Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this chapter we will be introduced to Scikit Learn's `RandomizedSearchCV` module.  Just like with `GridSearchCV`, it is a more efficient way of undertaking random search than doing it manually, and allows us to easily capture extra information on our training. \n",
    "\n",
    "Since we have already covered GridSearchCV, we don't need to learn a lot of new steps. Let's recall the steps for a Grid Search:\n",
    "\n",
    "1. Decide an algorithm to tune the hyperparameters for. (Sometimes called an estimator).\n",
    "2. Define which hyperparameters we will tune.\n",
    "3. Define a range of values for each hyperparameter.\n",
    "4. Set a cross-validation scheme.\n",
    "5. Define a scoring function so we can decide which grid square (model) was the best.\n",
    "6. Decide to include extra useful information or functions.\n",
    "\n",
    "There is only one difference when undertaking a random search:\n",
    "\n",
    "7. We need to decide how many hyperparameter combinations we will randomly sample to build models and then undertake this sampling before we model. \n",
    "\n",
    "And that's pretty much it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Comparing Scikit Learn Modules**\n",
    "\n",
    "It is therefore not a surprise to see how similar the functions are between the two Scikit Learn modules. See them here side by side:\n",
    "\n",
    "![Grid vs Random](assets/2-18.png)\n",
    "\n",
    "It may not be obvious what is different since there is far more the same than not! There is really only two key differences:\n",
    "\n",
    "* `n_iter`, which is the number of samples for the random search to take from our grid. In the previous example we did 300.\n",
    "* `param_distributions`, is slightly different from `aram_grid`. We can optionally give information on how to sample such as using a particular distribution we provide. If we just give a list as we have been doing, the default is to sample `'uniformly'` meaning every item in the list (combination) has equal chance of being chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Build a RandommizedCV Object**\n",
    "\n",
    "Now let's create a RandomSearchCV object, including the key changes we need to make. Creating a list of values and setting up the grid looks all very similar. \n",
    "\n",
    "```\n",
    "# Set up the sample space\n",
    "learn_rate_list = np.linspace(0.001, 2, 150)\n",
    "min_samples_leaf_list = list(range(1, 51))\n",
    "\n",
    "# Create the grid\n",
    "parameter_grid = {\n",
    "    'learning_rate': learning_rate_list,\n",
    "    'min_samples_leaf': min_samples_leaf_list}\n",
    "```\n",
    "\n",
    "We firstly create the lists of hyperparameter values using the `np.linspace()` and `range()` functions, then set up the dictionary grid of hyperparameter values. \n",
    "\n",
    "\n",
    "```\n",
    "# Define how many samples\n",
    "number_models = 10\n",
    "```\n",
    "\n",
    "The crucial small difference is at the end, defining how many samples to take. Now we create the random search object\n",
    "\n",
    "```\n",
    "# Create a random search object\n",
    "random_GBM_class = RandomizedSearchCV(\n",
    "    estimator = GradientBoostingClassifier(),\n",
    "    param_distributions = paramter_grid,\n",
    "    n_iter = number_models,\n",
    "    scoring = 'accuracy',\n",
    "    n_jobs = 4,\n",
    "    cv = 10,\n",
    "    refit = True,\n",
    "    return_train_score = True)\n",
    "    \n",
    "# Fit the object to our data\n",
    "random+GBM_class.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Note the slightly different name for the parameter grid, which is now called `'param_distributions'` as well as our new `n_iter` input for how many combinations to select and train models with. \n",
    "\n",
    "The attributes that form the output of RandomizedSearchCV are *exactly* the same as the GridSearchCV module However it would be interesting to see what numbers it sampled. We can visualize using the code from the previous chapter but where do we get details on the hyperparameters used?\n",
    "\n",
    "It is found in the `cv_results_` dictionary that was returned and in the relevant `param_` columns. \n",
    "\n",
    "Let's extract the `learning_rate` and `min_samples_leaf` used, to plot them:\n",
    "\n",
    "```\n",
    "rand_x = list(random_GBM_class.cv_results['param_learning_rate'])\n",
    "rand_y = list(random_GBM_class.cv_results['param_min_samples_leaf'])\n",
    "```\n",
    "\n",
    "Now we can plot our results. \n",
    "\n",
    "We set the x and y limits using NumPy's min and max functions over our list of hyperparameter values so that we can best see the coverage. \n",
    "\n",
    "```\n",
    "# Make sure we set the limits of Y and X appropriately\n",
    "x_lims = [np.min(learn_rate_list], np.max(learn_rate_list)]\n",
    "y_lims = [np.min(mins_samples_leaf_list], np.max(mins_samples_leaf_list)]\n",
    "```\n",
    "\n",
    "Then we plot these combinations as a scatter plot.\n",
    "\n",
    "```\n",
    "# Plot grid results\n",
    "plt.scatter(rand_y, rand_x, c=['blue']*10)\n",
    "plt.gca().set(xlabel='learn rate', ylabel='min_samples_leaf', title='Random Search Hyperparameters')\n",
    "plt.gca().set_xlim(x_lims)\n",
    "plt.gca().set_ylim(y_lims)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "For computational efficiency we only ran 10 models this time. Else it would take a while! \n",
    "\n",
    "\n",
    "![Random plot](assets/2-7.png)\n",
    "\n",
    "\n",
    "We will notice this is plot looks very similar to what plotted before as hyperparameter combinations without actually undertaking the model creation. Now we actually ran one, it looks very similar. Random search has a wide coverage of area for possible hyperparameters but it is a very pachy coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The RandomizedSeachCV Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Just like the `GridSearchCV` library from Scikit Learn, `RandomizedSearchCV` provides many useful features to assist with efficiently undertaking a random search. We're going to create a `RandomizedSearchCV` object, making the small adjustment needed from the `GridSearchCV` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The desired options are:\n",
    "\n",
    "* A default Gradient Boosting Classifier Estimator\n",
    "* 5-fold cross validation\n",
    "* Use accuracy to score the models\n",
    "* Use 4 cores for processing in parallel\n",
    "* Ensure you refit the best model and return training scores\n",
    "* Randomly sample 10 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The hyperparameter grid should be for `learning_rate` (150 values between 0.1 and 2) and `min_samples_leaf` (all values between 20 and 65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:26.250704Z",
     "start_time": "2020-02-19T18:57:26.237740Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "param_grid = {'learning_rate': np.linspace(0.1, 2, 150), 'min_samples_leaf': list(range(20, 65))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:57:26.266685Z",
     "start_time": "2020-02-19T18:57:26.253695Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Create a random search object\n",
    "random_GBM_class = RandomizedSearchCV(\n",
    "    estimator = GradientBoostingClassifier(random_state=42),\n",
    "    param_distributions = param_grid,\n",
    "    n_iter = 10,\n",
    "    scoring='accuracy', n_jobs=4, cv = 5, refit=True, return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:58:53.916148Z",
     "start_time": "2020-02-19T18:57:26.268660Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fit to the training data\n",
    "random_GBM_class.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:58:53.931109Z",
     "start_time": "2020-02-19T18:58:53.918141Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Print the values used for both hyperparameters\n",
    "print(random_GBM_class.cv_results_['param_learning_rate'])\n",
    "print(random_GBM_class.cv_results_['param_min_samples_leaf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### RandomSearchCV in Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's practice building a RandomizedSearchCV object using Scikit Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The desired options are:\n",
    "\n",
    "* A RandomForestClassifier Estimator with default 80 estimators\n",
    "* 3-fold cross validation\n",
    "* Use AUC to score the models\n",
    "* Use 4 cores for processing in parallel\n",
    "* Ensure you refit the best model and return training scores\n",
    "* Randomly sample 5 models for processing efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The hyperparameter grid should be for `max_depth` (all values between and including 5 and 25) and `max_features` ('auto' and 'sqrt')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:58:53.946068Z",
     "start_time": "2020-02-19T18:58:53.935099Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "param_grid = {'max_depth': list(range(5,26)), 'max_features': ['auto' , 'sqrt']} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:58:53.961029Z",
     "start_time": "2020-02-19T18:58:53.948064Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a random search object\n",
    "random_rf_class = RandomizedSearchCV(\n",
    "    estimator = RandomForestClassifier(n_estimators=80, random_state=42),\n",
    "    param_distributions = param_grid, n_iter = 5,\n",
    "    scoring='roc_auc', n_jobs=4, cv = 3, refit=True, return_train_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:59:07.407804Z",
     "start_time": "2020-02-19T18:58:53.963024Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fit to the training data\n",
    "random_rf_class.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:59:07.423278Z",
     "start_time": "2020-02-19T18:59:07.409795Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Print the values used for both hyperparameters\n",
    "print(random_rf_class.cv_results_['param_max_depth'])\n",
    "print(random_rf_class.cv_results_['param_max_features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Grid and Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapters we looked at both Grid and Random search in depth. So, it is a good idea to stop and reinforce our knowledge on these two valuable techniques.\n",
    "\n",
    "There are a number of similarities for these techniques:\n",
    "\n",
    "* Both are automated ways of tuning different hyperparameters.\n",
    "* For both we set the grid to sample from (which hyperparameters and values for each).\n",
    "* For both we need to think carefully about which hyperparameters and values to sample from as our model will only be as good as the grid we set!\n",
    "* And for both we set a cross-validation scheme and scoring function.\n",
    "\n",
    "There are also some key differences between these two techniques:\n",
    "\n",
    "| Grid Search | Random Search |\n",
    "| ----------- | ------------- |\n",
    "| Exhaustively tries all the combinations within<br> the sample space | Randomly selects a subset combinations<br> within the sample space (that we must specify) |\n",
    "| No sampling methodology | Can select a sampling methodology (other<br> than `uniform` which is default |\n",
    "| More computationally expensive | Less computationally expensive |\n",
    "| Guaranteed to find the best score in the sample space | Not guaranteed to find the best score in<br> the sample space (but likely to find a *good* one *faster* |\n",
    "\n",
    "**So which technique should we use?**\n",
    "\n",
    "As we often say in data science - 'It depends'. However there are some things we can consider. \n",
    "\n",
    "* Firstly, the more data we have, the stronger the argument for random search.\n",
    "* More hyperparameters and values to try also means random search might be a better option. \n",
    "* Additionally if we don't have a lot of time or computing power, random search will be more economical. Remember, random search has more chance of getting a good result faster, even if not the absolute best.\n",
    "\n",
    "Let's do some examples to illustrate the differences between grid and random search visually and conceptually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Grid and Random Search Side by Side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Visualizing the search space of random and grid search together allows us to easily see the coverage that each technique has and therefore brings to life their specific advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example, we will sample hyperparameter combinations in a grid search way as well as a random search way, then plot these to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:59:07.439259Z",
     "start_time": "2020-02-19T18:59:07.426271Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sample grid coordinates\n",
    "grid_combinations_chosen = combinations_list[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:59:07.471182Z",
     "start_time": "2020-02-19T18:59:07.442226Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Print result\n",
    "grid_combinations_chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:59:07.487147Z",
     "start_time": "2020-02-19T18:59:07.473176Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a list of sample indexes\n",
    "sample_indexes = list(range(0,len(combinations_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:59:07.502067Z",
     "start_time": "2020-02-19T18:59:07.490100Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Randomly sample 300 indexes\n",
    "random_indexes = np.random.choice(sample_indexes, 300, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:59:07.517025Z",
     "start_time": "2020-02-19T18:59:07.504061Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Use indexes to create random sample\n",
    "random_combinations_chosen = [combinations_list[index] for index in random_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:59:07.533009Z",
     "start_time": "2020-02-19T18:59:07.520033Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def visualize_search(grid_combinations_chosen, random_combinations_chosen):\n",
    "    grid_y, grid_x = [x[0] for x in grid_combinations_chosen], [x[1] for x in grid_combinations_chosen]\n",
    "    rand_y, rand_x = [x[0] for x in random_combinations_chosen], [x[1] for x in random_combinations_chosen]\n",
    "\n",
    "    # Plot all together\n",
    "    plt.scatter(grid_y + rand_y, grid_x + rand_x, c=['red']*300 + ['blue']*300)\n",
    "    plt.gca().set(xlabel='learn_rate', ylabel='min_samples_leaf', title='Grid and Random Search Hyperparameters')\n",
    "    plt.gca().set_xlim([0.01, 3.0])\n",
    "    plt.gca().set_ylim([5, 24])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-19T18:59:07.765429Z",
     "start_time": "2020-02-19T18:59:07.534980Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Call the function to produce the visualization\n",
    "visualize_search(grid_combinations_chosen, random_combinations_chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can really see how a grid search will cover a small area completely whilst random search will cover a much larger area but not completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "**[Week 5 - Data Preprocessing and Hyperparameter Tuning](https://radu-enuca.gitbook.io/ml-challenge/preprocessing-and-tuning)**\n",
    "\n",
    "*Have questions or comments? Visit the ML Challenge Mattermost Channel.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
