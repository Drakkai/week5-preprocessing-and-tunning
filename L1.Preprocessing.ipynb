{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ML-Challenge/week5-preprocessing-and-tunning/blob/master/L1.Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lesson covers the basics of how and when to perform data preprocessing. This essential step in any machine learning project is when we get our data ready for modeling. Between importing and cleaning the data and fitting the machine learning model is when preprocessing comes into play. We'll learn how to standardize the data so that it's in the right form for the model, create new features to best leverage the information in the dataset, and select the best features to improve the model fit. Finally, we'll have some practice preprocessing by getting a dataset on UFO sightings ready for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.017074Z",
     "start_time": "2020-02-16T17:50:51.009105Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download lesson datasets\n",
    "# Required if you're using Google Colab\n",
    "!wget \"https://github.com/ML-Challenge/week5-preprocessing-and-tunning/raw/master/datasets.zip\"\n",
    "!unzip -o datasets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.541783Z",
     "start_time": "2020-02-16T17:50:51.048543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import utils\n",
    "# We'll be using this module throughout the lesson\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.759200Z",
     "start_time": "2020-02-16T17:50:51.544741Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# and setting the size of all plots.\n",
    "plt.rcParams['figure.figsize'] = [11, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Introduction to Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll learn exactly what it means to `preprocess` data. We'll take the first steps in any preprocessing journey, including exploring data types and dealing with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## What is data preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing comes after we've cleaned up our data and after we've done some exploratory analysis to understand our dataset. Once we understand our dataset, we'll probably have some idea about how we want to model our data. \n",
    "\n",
    "Machine learning models in Python require numerical input, so if our dataset has categorical variables, we'll need to transform them. Think of data preprocessing as a prerequisite for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first steps we can take to preprocess our data is to **remove missing data**. There's a lot of ways to deal with missing data, but here we're only going to cover ways to remove either columns or rows with missing data.\n",
    "\n",
    "If we wanted to drop all rows from a dataframe that contain missing values, we can do that with `dropna`. We can drop specific rows by passing index labels to the `drop` function, which defaults to dropping rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data - columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we'll want to focus on dropping a particular column, especially if all or most of its values are missing. We can\n",
    "use the `drop` method as well, though the parameters are different. The first parameter is the column name. We have to specify `axis=1` in order to designate that we want to drop a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset comprised of volunteer information from New York City. The dataset has a number of features, but we want to get rid of features that have at least 3 missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many features are in the original dataset, and how many features are in the set after columns with at least 3 missing values are removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.790117Z",
     "start_time": "2020-02-16T17:50:51.761197Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.821037Z",
     "start_time": "2020-02-16T17:50:51.792110Z"
    }
   },
   "outputs": [],
   "source": [
    "volunteer_clean = utils.volunteer.dropna(axis=1, thresh=3)\n",
    "volunteer_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data - rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to drop rows where data is missing in a particular column? We can do this with the help of boolean\n",
    "indexing, which is a way to filter a dataframe based on certain values. Instead of indexing a dataframe using column or\n",
    "row names, we can set a condition to filter our dataframe by to return a specific set of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the `volunteer` dataset again, we want to drop rows where the `category_desc` column values are missing. We're going to do this using boolean indexing, by checking to see if we have any null values, and then filtering the dataset so that we only have rows with those values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at how many null values we have in column `category_desc`, using `isnull` to get null values and then using sum to output a count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.835989Z",
     "start_time": "2020-02-16T17:50:51.823031Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check how many values are missing in the category_desc column\n",
    "print(utils.volunteer['category_desc'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter those out, we can simply use the notnull method on column `category_desc` as a boolean index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.850949Z",
     "start_time": "2020-02-16T17:50:51.837958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset the volunteer dataset\n",
    "volunteer_subset = utils.volunteer[utils.volunteer['category_desc'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.865912Z",
     "start_time": "2020-02-16T17:50:51.851946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the shape of the subset\n",
    "print(volunteer_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: we can use boolean indexing to effectively subset DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Working with data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've reviewed some Pandas basics, we need to start thinking about other steps we have to take in order to\n",
    "prepare the data. One of these steps is to think about the types that are present in our dataset, because\n",
    "we'll likely have to transform some of these columns to other types later on. Let's take a deeper look at types, as well\n",
    "as how to convert column types in our dataset. Pandas datatypes are similar to native python types, but there are a couple of things to be aware of. The most common types we'll be working with are `object` (string values or is of mixed types), `int64` (equivalent to the Python integer type), and `float64` (equivalent to the float type) types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking another look at the dataset comprised of volunteer information from New York City, we want to know what types we'll be working with as we start to do more preprocessing. Which data types are present in the volunteer dataset?\n",
    "\n",
    "> **Note:** Recall that we can check the types of a dataframe by using the `dtypes` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.881871Z",
     "start_time": "2020-02-16T17:50:51.868876Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have int, float and object (string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting column types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we'll start working with a dataset that has an incorrect column type: maybe a numerical column was written out into a csv as a string, and when we try to work with that column, numerical operations won't work. It's also good to be as sure as we can that the column type we want to convert to is representative of the whole column. The object type can represent a column that includes both string and numeric types.\n",
    "\n",
    "Let's take a look at how to adjust the type of a column if the type that pandas has inferred upon reading in the file is incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the `volunteer` dataset types, we'll see that the column `hits` is type object. But, if we actually look at the column, we'll see that it consists of integers. Let's convert that column to type int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.896826Z",
     "start_time": "2020-02-16T17:50:51.883836Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the head of the hits column\n",
    "print(utils.volunteer[\"hits\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the type using the `astype` method and passing in the type we want to convert it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.912783Z",
     "start_time": "2020-02-16T17:50:51.898796Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the hits column to type int\n",
    "utils.volunteer[\"hits\"] = utils.volunteer[\"hits\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.927751Z",
     "start_time": "2020-02-16T17:50:51.914758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the dtypes of the dataset\n",
    "print(utils.volunteer.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most necessary steps for preprocessing, is splitting up our data into training and test sets. We do this to avoid the issue of overfitting. If we train a model on our entire set of data, we won't have any way to test and validate our model because the model will essentially know the dataset by heart. Holding out a test set allows us to preserve some data the model hasn't seen yet.\n",
    "\n",
    "In scikit learn, we can split our dataset by using the `train_test_split` function. The function shuffles up our dataset and then randomly splits it. By default, the function will split 75% of the data into the training set and 25% into the test set. In many scenarios, the default splitting parameters will work well. However, if our labels have an uneven distribution, our test and training sets might not be representative samples of our dataset and could bias the model we're trying to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `volunteer` dataset, we're thinking about trying to predict the `category_desc` variable using the other features in the dataset. First, though, we need to know what the class distribution (and imbalance) is for that label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which descriptions occur less than 50 times in the volunteer dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.942710Z",
     "start_time": "2020-02-16T17:50:51.928747Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer['category_desc'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Emergency Prepardness and Environment occur less than 50 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good technique for sampling more accurately when we have imbalanced classes is **stratified sampling**, which is a way of sampling that takes into account the distribution of\n",
    "classes or features in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the distribution of variables in the `category_desc` column in the `volunteer` dataset is uneven. If we wanted to train a model to try to predict `category_desc`, we would want to train the model on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.958661Z",
     "start_time": "2020-02-16T17:50:51.944677Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a data with all columns except category_desc\n",
    "volunteer_X = utils.volunteer.drop('category_desc', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.974627Z",
     "start_time": "2020-02-16T17:50:51.960631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a category_desc labels dataset\n",
    "volunteer_y = utils.volunteer[['category_desc']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a really easy way to do this in scikit learn using the train test split function. The function comes with a `stratify` parameter, and to stratify according to class labels, we just pass in our `volunteer_y` to that parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.409469Z",
     "start_time": "2020-02-16T17:50:51.976590Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use stratified sampling to split up the dataset according to the volunteer_y dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the distribution of classes for our training and test labels, we can see the distribution of classes is in accordance with the original `category_desc` class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.425390Z",
     "start_time": "2020-02-16T17:50:52.411429Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the category_desc counts on the training y labels\n",
    "print(y_train['category_desc'].value_counts())\n",
    "\n",
    "# Print out the category_desc counts on the test y labels\n",
    "print(y_test['category_desc'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Standardizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the lesson is all about standardizing data. Often a model will make some assumptions about the distribution or scale of the features. Standardization is a way to make the data fit these assumptions and improve the algorithm's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## What is data standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible that we'll come across datasets with lots of numerical noise built in, such as lots of variance or differently-scaled data. The preprocessing solution for that is standardization. \n",
    "\n",
    "Standardization is a preprocessing method used to transform continuous data to make it look normally distributed. In scikit-learn, this is often a necessary step, because many models assume that the data we are training on is normally distributed, and if it isn't, we risk biasing our model.\n",
    "\n",
    "We can standardize our data in different ways, but in this lesson we're going to talk about two methods: `log normalization` and `scaling`. It's also important to note that standardization is a preprocessing method applied to continuous, numerical data. We'll learn methods for dealing with categorical data later in the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to standardize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few different scenarios in which we want to standardize our data. First, if we're working with any kind of model that uses a linear distance metric or operates in a linear space like k-nearest neighbors, linear regression, or k-means clustering, the model is assuming that the data and features we're giving it are related in a linear fashion,\n",
    "or can be measured with a linear distance metric. There are a number of models that deal with nonlinear spaces, but for those models that are in a linear space, the data must also be in that space.\n",
    "\n",
    "The case when a feature or features in our dataset have high variance is related to this. This could bias a model that assumes the data is normally distributed. If a feature in our dataset has a variance that's an order of magnitude or more greater than the other features, this could impact the model's ability to learn from other features in the dataset.\n",
    "\n",
    "Modeling a dataset that contains continuous features that are on different scales is another scenario to watch out for. For example, consider a dataset that contains a column related to height and another related to weight. In order to compare these features, they must be in the same linear space, and therefore must be standardized in some way. \n",
    "\n",
    "All of these scenarios assume we're working with a model that makes some kind of linearity assumptions. There are a number of models that are perfectly fine operating in a nonlinear space or do a certain amount of standardization upon input, but that's outside the scope of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned when it is appropriate to standardize our data, which of these scenarios would we **NOT** want to standardize?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "1. A column we want to use for modeling has extremely high variance.\n",
    "2. We have a dataset with several continuous columns on different scales and we'd like to use a linear model to train the data.\n",
    "3. The models we're working with use some sort of distance metric in a linear space, like the Euclidean metric.\n",
    "4. Our dataset is comprised of categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.441348Z",
     "start_time": "2020-02-16T17:50:52.427386Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use 1, 2, 3 or 4 as a parameter\n",
    "utils.when_to_standardize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling without normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what might happen to our model's accuracy if we try to model data without doing some sort of standardization first. Here we have a subset of the `wine` dataset. One of the columns, `Proline`, has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which we'll learn about later in this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.473305Z",
     "start_time": "2020-02-16T17:50:52.443344Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.489253Z",
     "start_time": "2020-02-16T17:50:52.476255Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a subset of data\n",
    "wine_X = utils.wine[['Proline', 'Total phenols', 'Hue', 'Nonflavanoid phenols']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.505213Z",
     "start_time": "2020-02-16T17:50:52.492212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Type labels dataset\n",
    "wine_y = utils.wine['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.520161Z",
     "start_time": "2020-02-16T17:50:52.507206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_X, wine_y, stratify=wine_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.581002Z",
     "start_time": "2020-02-16T17:50:52.522132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the k-nearest neighbors model to the training data\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.596966Z",
     "start_time": "2020-02-16T17:50:52.582971Z"
    }
   },
   "outputs": [],
   "source": [
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy score is pretty low. Let's explore methods to improve this score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Log normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log normalization is a method for standardizing our data that can be useful when we have a particular column with **high variance**. As we saw in the previous section's\n",
    "examples, training a k-nearest neighbors classifier on that subset of the wine dataset didn't get a very high accuracy score. This is because within that subset, the `Proline` colummn has extremely high variance, which is affecting the accuracy of the classifier.\n",
    "\n",
    "Log normalization is a good strategy:\n",
    "* when we care about relative changes in a linear model\n",
    "* when we still want to capture the magnitude of change\n",
    "* when we want to keep everything in the positive space.\n",
    "\n",
    "It's a nice way to minimize the variance of a column and make it comparable to other columns for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the variance of the columns in the wine dataset and see which column is a candidate for normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.612914Z",
     "start_time": "2020-02-16T17:50:52.598927Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.wine.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `Proline` column has an extremely high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log normalization in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of log normalization we're going to work with in Python takes the natural log of each number in the left hand column, which is simply the exponent we would raise above the mathematical constant e (approximately equal to 2.718) to get that number.\n",
    "\n",
    "Now that we know that the `Proline` column in our wine dataset has a large amount of variance, let's log normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.627876Z",
     "start_time": "2020-02-16T17:50:52.614889Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the variance of the Proline column\n",
    "print(utils.wine['Proline'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.642835Z",
     "start_time": "2020-02-16T17:50:52.629870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the log normalization function to the Proline column\n",
    "utils.wine['Proline_log'] = np.log(utils.wine['Proline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.657783Z",
     "start_time": "2020-02-16T17:50:52.645803Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the variance of the normalized Proline column\n",
    "print(utils.wine['Proline_log'].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `np.log()` function is an easy way to log normalize a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.673754Z",
     "start_time": "2020-02-16T17:50:52.659765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a subset of data\n",
    "wine_X = utils.wine[['Proline_log', 'Total phenols', 'Hue', 'Nonflavanoid phenols']]\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_X, wine_y, stratify=wine_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.689721Z",
     "start_time": "2020-02-16T17:50:52.675743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.704670Z",
     "start_time": "2020-02-16T17:50:52.691681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Scaling data for feature comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is feature scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is a method of standardization that's most useful when we're working with a dataset that contains continuous features that are on different scales, and we're using a model that operates in some sort of linear space (like linear regression or k-nearest neighbors).\n",
    "\n",
    "Feature scaling transforms the features in our dataset so they have a mean of 0 and a variance of 1. This will make it easier to linearly compare features. This is a requirement for many models in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data - investigating columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the `Ash`, `Alcalinity of ash`, and `Magnesium` columns in the wine dataset to train a linear model, but it's possible that these columns are all measured in different ways, which would bias a linear model. Using `describe()` to return descriptive statistics about this dataset, which of the following statements are true about the scale of data in these columns?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "1. The max of `Ash` is 3.23, the max of `Alcalinity of ash` is 30, and the max of `Magnesium` is 162.\n",
    "2. The means of `Ash` and `Alcalinity of ash` are less than 20, while the mean of `Magnesium` is greater than 90.\n",
    "3. The standard deviations of `Ash` and `Alcalinity of ash` are equal.\n",
    "4. 1 and 2 are true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.767509Z",
     "start_time": "2020-02-16T17:50:52.707638Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.wine.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data - standardizing columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know that the `Ash`, `Alcalinity of ash`, and `Magnesium` columns in the wine dataset are all on different scales, let's standardize them in a way that allows for use in a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has a variety of scaling capabilities, but we're only going to focus on the `StandardScaler` class, imported from preprocessing. It works by removing the mean and scaling each feature to have unit variance. There's a simpler scale function in scikit-learn, but the benefit of using the `StandardScaler` object is that we can apply the same transformation on other data, like a test set, or new data that's part of the same set, for example, without having to rescale everything. So once we have the standard scaler method, we can apply the `fit_transform` function on the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.783466Z",
     "start_time": "2020-02-16T17:50:52.769505Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import StandardScaler from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.798395Z",
     "start_time": "2020-02-16T17:50:52.785463Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the scaler\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.818421Z",
     "start_time": "2020-02-16T17:50:52.802385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a subset of the DataFrame we want to scale \n",
    "wine_subset = utils.wine[['Ash', 'Alcalinity of ash', 'Magnesium']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.833914Z",
     "start_time": "2020-02-16T17:50:52.820924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the scaler to the DataFrame subset\n",
    "wine_subset_scaled = ss.fit_transform(wine_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.865834Z",
     "start_time": "2020-02-16T17:50:52.835887Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(wine_subset_scaled, columns=['Ash', 'Alcalinity of ash', 'Magnesium']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, running `fit_transform` during preprocessing will both fit the method to the data as well as transform the data in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Standardized data and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned a couple of different methods for standardization, it's time to put this into practice with modeling. As mentioned before, many models in scikit-learn require our data to be scaled appropriately across columns, otherwise we risk biasing our results. The last part of this section will be dedicated to modeling data on both unscaled and scaled data, so we can see the difference in model performance. The model we're going to use is k-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN on non-scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at the accuracy of a K-nearest neighbors model on the `wine` dataset without standardizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.881784Z",
     "start_time": "2020-02-16T17:50:52.867799Z"
    }
   },
   "outputs": [],
   "source": [
    "X = utils.wine.drop(['Type', 'Proline_log'], axis=1)\n",
    "y = utils.wine['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.897755Z",
     "start_time": "2020-02-16T17:50:52.883784Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.913708Z",
     "start_time": "2020-02-16T17:50:52.898747Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.928669Z",
     "start_time": "2020-02-16T17:50:52.915706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN on scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score on the unscaled `wine` dataset was decent, but we can likely do better if we scale the dataset. The process is mostly the same as the previous example, with the added step of scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.944629Z",
     "start_time": "2020-02-16T17:50:52.931630Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the scaling method.\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.960576Z",
     "start_time": "2020-02-16T17:50:52.946620Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the scaling method to the dataset used for modeling.\n",
    "X_scaled = ss.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.975514Z",
     "start_time": "2020-02-16T17:50:52.962546Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.991467Z",
     "start_time": "2020-02-16T17:50:52.977504Z"
    }
   },
   "outputs": [],
   "source": [
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The increase in accuracy is worth the extra step of scaling the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll learn about feature engineering. We'll explore different ways to create new, more useful, features from the ones already in the dataset. We'll see how to encode, aggregate, and extract information from both numerical and textual features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## What is Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is the creation of new features based on existing features, and it adds information to our dataset that is useful in some way: it adds features useful for our prediction or clustering task, or it sheds insight into relationships between features. Real world data is often not neat and tidy, and in addition to preprocessing steps like standardization, we'll likely have to extract and expand information that exists in the columns in our dataset.\n",
    "\n",
    "Feature engineering is also something that is very dependent on the particular dataset we're analyzing.\n",
    "\n",
    "There are a variety of scenarios in which we might want to engineer features from existing data. An extremely common one is with **text data**. For example, if we're building some kind of natural language processing model, we'll have to create a vector of the words in our dataset.\n",
    "\n",
    "Another scenario might also be related to string data: maybe we have a column which  records people's favorite colors. In order to feed this information into a model in scikit-learn, we'll have to encode this information numerically.\n",
    "\n",
    "Another common example is with timestamps. We might see a full timestamp that includes the time down to the second or millisecond, which might be much too granular for a prediction task, so we'll want to create a new column with the day or the month. Perhaps a column contains a list of some kind: test scores, or running times, and maybe it's more useful to use an average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering knowledge test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned about feature engineering, which of the following examples are good candidates for creating new features?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "1. A column of timestamps\n",
    "2. A column of newspaper headlines\n",
    "3. A column of weight measurements\n",
    "4. 1 and 2\n",
    "5. None of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.022385Z",
     "start_time": "2020-02-16T17:50:53.016403Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use 1,2,3,4 or 5 as parameter\n",
    "utils.feature_engineering_puzzle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying areas for feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an exploratory look at the `volunteer` dataset. Which of the following columns would we want to perform a feature engineering task on?\n",
    "\n",
    "**Posible Answers**\n",
    "\n",
    "1. `vol_requests`\n",
    "2. `title`\n",
    "3. `created_date`\n",
    "4. `category_desc`\n",
    "5. 2,3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.099212Z",
     "start_time": "2020-02-16T17:50:53.067267Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.115142Z",
     "start_time": "2020-02-16T17:50:53.110187Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use 1,2,3,4 or 5 as parameter\n",
    "utils.identity_features_puzzle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Encoding categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because models in scikit-learn require numerical input, if our dataset contains categorical variables, we'll have to encode them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables - binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the `hiking` dataset. There are several columns here that need encoding, one of which is the `Accessible` column, which needs to be encoded in order to be modeled. `Accessible` is a binary feature, so it has two values - either `Y` or `N` - so it needs to be encoded into 1s and 0s. We'll use scikit-learn's `LabelEncoder` method to do that transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.176974Z",
     "start_time": "2020-02-16T17:50:53.152073Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.hiking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.207913Z",
     "start_time": "2020-02-16T17:50:53.195923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.237842Z",
     "start_time": "2020-02-16T17:50:53.230890Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.284720Z",
     "start_time": "2020-02-16T17:50:53.264771Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the encoding to the \"Accessible\" column\n",
    "utils.hiking['Accessible_enc'] = enc.fit_transform(utils.hiking['Accessible'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.331627Z",
     "start_time": "2020-02-16T17:50:53.305695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare the two columns\n",
    "utils.hiking[['Accessible', 'Accessible_enc']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit_transform()` is a good way to both fit an encoding and transform the data in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables - one-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the columns in the volunteer dataset, `category_desc`, gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically. We'll use Pandas' `get_dummies()` function to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.362510Z",
     "start_time": "2020-02-16T17:50:53.345589Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(utils.volunteer['category_desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.408386Z",
     "start_time": "2020-02-16T17:50:53.393459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the encoded columns\n",
    "category_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_dummies()` is a simple and quick way to encode categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Engineering numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we may have a dataset filled with numerical features, they may need a little bit of feature engineering to properly prepare for modeling. In this section, we'll talk about aggregate statistics as well as dates and how engineering numerical features can add value to our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had, say, a collection of features related to a single feature, like temperature or running time, we might want to take an average or median to use as a feature for modeling instead. A common method of feature engineering is to take an aggregate of a set of numbers to use in place of those features. This can be helpful in reducing the dimensionality\n",
    "of our feature space, or perhaps we simply don't need multiple similar values that are close in distance to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering numerical features - taking an average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good use case for taking an aggregate statistic to create a new feature is to take the mean of columns. Here, we have a DataFrame of running times named `running_times_5k`. For each `name` in the dataset, take the mean of their 5 run times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.456261Z",
     "start_time": "2020-02-16T17:50:53.440332Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.running_times_5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.502137Z",
     "start_time": "2020-02-16T17:50:53.486212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of the columns to average\n",
    "run_columns = ['run1', 'run2', 'run3', 'run4', 'run5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.547024Z",
     "start_time": "2020-02-16T17:50:53.522130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use apply to create a mean column\n",
    "utils.running_times_5k[\"mean\"] = utils.running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.591904Z",
     "start_time": "2020-02-16T17:50:53.564984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the results\n",
    "utils.running_times_5k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambdas are especially helpful for operating across columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering numerical features - datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several columns in the `volunteer` dataset comprised of datetimes. Let's take a look at the `start_date_date` column and extract just the month to use as a feature for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.623315Z",
     "start_time": "2020-02-16T17:50:53.605901Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer[['start_date_date', 'end_date_date']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.717614Z",
     "start_time": "2020-02-16T17:50:53.646232Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, convert string column to date column\n",
    "utils.volunteer[\"start_date_converted\"] = pd.to_datetime(utils.volunteer[\"start_date_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.733573Z",
     "start_time": "2020-02-16T17:50:53.720576Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer[['start_date_date', 'start_date_converted']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.749530Z",
     "start_time": "2020-02-16T17:50:53.736534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract just the month from the converted column\n",
    "utils.volunteer['start_date_month'] = utils.volunteer['start_date_converted'].apply(lambda row: row.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.781445Z",
     "start_time": "2020-02-16T17:50:53.767475Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the converted and new month columns\n",
    "utils.volunteer[['start_date_date', 'start_date_converted', 'start_date_month']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use attributes like `.day` to get the day and `.year` to get the year from datetime columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though text data is a little more complicated to work with, there's a lot of useful feature engineering we can do with it. One method is to extract the pieces of information that we need: maybe part of a string, or extracting a number, and transforming it into a feature. We can also transform the text itself into features, for use with natural language processing methods or prediction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering features from strings - extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Length` column in the `hiking` dataset is a column of strings, but contained in the column is the mileage for the hike. We're going to extract this mileage using regular expressions, and then use a lambda in Pandas to apply the extraction to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.828802Z",
     "start_time": "2020-02-16T17:50:53.808390Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.hiking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.860740Z",
     "start_time": "2020-02-16T17:50:53.849745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write a pattern to extract numbers and decimals\n",
    "import re\n",
    "\n",
    "def return_mileage(length):\n",
    "    if length is not None:\n",
    "        pattern = re.compile(r\"\\d+\\.\\d+\")\n",
    "\n",
    "        # Search the text for matches\n",
    "        mile = re.match(pattern, length)\n",
    "\n",
    "        # If a value is returned, use group(0) to return the found value\n",
    "        if mile is not None:\n",
    "            return float(mile.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.892629Z",
     "start_time": "2020-02-16T17:50:53.884668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the function to the Length column\n",
    "utils.hiking[\"Length_num\"] = utils.hiking[\"Length\"].apply(lambda row: return_mileage(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.938532Z",
     "start_time": "2020-02-16T17:50:53.923582Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at both columns\n",
    "utils.hiking[[\"Length\", \"Length_num\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're working with text, we might want to model it in some way. In order to do that, we'll need to vectorize the text and transform it into a numerical input that scikit-learn can use.\n",
    "\n",
    "Let's transform the `volunteer` dataset's `title` column into a text vector, to use in a prediction task in the next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.969457Z",
     "start_time": "2020-02-16T17:50:53.962470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take the title text\n",
    "title_text = utils.volunteer.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to create a `tf/idf` vector. `tf/idf` is a way of vectorizing text that reflects how important a word is in a document beyond how frequently it occurs. It stands for term frequency / inverse document frequency and places the weight on words that are ultimately more significant in the entire corpus of words.\n",
    "\n",
    "Creating `tf/idf` vectors is straightforward in scikit-learn, and we can use the `TfidfVectorizer` class to do it. In order to vectorize `title_text`, we can simply pass it into `TfidfVectorizer`'s `fit_transform` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.016326Z",
     "start_time": "2020-02-16T17:50:54.001341Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the vectorizer method\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.047251Z",
     "start_time": "2020-02-16T17:50:54.036279Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using tf/idf vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've encoded the`volunteer` dataset's `title` column into `tf/idf` vectors, let's use those vectors to try to predict the `category_desc` column. Notice that we have to run the `toarray()` method on the tf/idf vector, in order to get in it the proper format for scikit-learn.\n",
    "\n",
    "We'll use a Naive Bayes classifier, which is based on Bayes' theorem of conditional probability, and performs well on text classification tasks. Naive Bayes treats each feature as independent from the others, which can be a naive assumption, but this works out well on text data. Because each feature is treated independently, this classifier works well on high-dimensional data and isb very efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.094127Z",
     "start_time": "2020-02-16T17:50:54.076165Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = utils.volunteer[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.140003Z",
     "start_time": "2020-02-16T17:50:54.120024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's use Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.187875Z",
     "start_time": "2020-02-16T17:50:54.160916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the model's accuracy\n",
    "nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model doesn't score very well. We'll work on selecting the best features for modeling in the next part of the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Selecting features for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section goes over a few different techniques for selecting the most important features from the dataset. We'll learn how to drop redundant features, work with text vectors, and reduce the number of features in the dataset using principal component analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is a method of selecting features from our feature set to be used for modeling. It draws from a set of existing features, so it's different than feature\n",
    "engineering because it doesn't create new features.\n",
    "\n",
    "The overarching goal of feature selection is to improve our model's performance. Perhaps our existing feature set is much too large, or some of the features we're working with are unnecessary. There are different ways we can perform feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps to git rid of noise in our model. Maybe we have redundant features, like both latitude and longitude and city and state, that are adding noise. Or maybe we have features that are strongly statistically correlated, which breaks the assumptions of certain models and thus impacts model performance. If we're working with text vectors, we'll want to use those tf-idf vectors to determine which set of words to train our model on. And finally, if our feature set is large, it may be beneficial to use dimensionality reduction to combine and reduce the number of features in our dataset in a way that also reduces the overall variance.\n",
    "\n",
    "Let's say we had finished standardizing our data and creating new features. Which of the following scenarios is **NOT** a good candidate for feature selection?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "1. Several columns of running times that have been averaged into a new column.\n",
    "2. A text field that hasn't been turned into a tf/idf vector yet.\n",
    "3. A column of text that has already had a float extracted out of it.\n",
    "4. A categorical field that has been one-hot encoded.\n",
    "5. The dataset contains columns related to whether something is a fruit or vegetable, the name of the fruit or vegetable, and the scientific name of the plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.219792Z",
     "start_time": "2020-02-16T17:50:54.205829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct! The text field needs to be vectorized before we can eliminate it, otherwise we might miss out on important data.\n"
     ]
    }
   ],
   "source": [
    "# Use 1,2,3,4 or 5 as parameter\n",
    "utils.feature_selction_puzzle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Removing redundant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the easiest ways to determine if a feature is unnecessary is if it is redundant in some way. For example, if it exists in another form as another feature, or if two features are very strongly correlated. Sometimes, when we create features through feature engineering, we end up duplicating existing features in some way.\n",
    "\n",
    "Some redundant features can be identified manually, by simply having an understanding of the  features in our dataset. Like the machine learning process in general, feature selection is an iterative process. We might try removing some features only to find it doesn't improve our model's performance, and we might have to reassess our selection choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting relevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's identify the redundant columns in the `volunteer_processed` dataset and perform feature selection on the dataset to return a DataFrame of the relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we explore the `volunteer` dataset, we'll see three features which are related to location: `locality`, `region`, and `postalcode`. They contain repeated information, so it would make sense to keep only one of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.267662Z",
     "start_time": "2020-02-16T17:50:54.250707Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer[['locality', 'region', 'postalcode']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also features that have gone through the feature engineering process: columns like 'Education' and 'Emergency Preparedness' are a product of encoding the categorical variable `category_desc`, so `category_desc` itself is redundant now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.313540Z",
     "start_time": "2020-02-16T17:50:54.297575Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.345422Z",
     "start_time": "2020-02-16T17:50:54.335477Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.376340Z",
     "start_time": "2020-02-16T17:50:54.370385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of redundant column names to drop\n",
    "to_drop = [\"category_desc\", \"created_date\", \"locality\", \"region\", \"vol_requests\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.423239Z",
     "start_time": "2020-02-16T17:50:54.403299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop those columns from the dataset\n",
    "volunteer_subset = utils.volunteer_processed.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.471120Z",
     "start_time": "2020-02-16T17:50:54.447184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the head of the new dataset\n",
    "volunteer_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A clear situation in which we'd want to drop features is when they are statistically correlated, meaning they move together directionally. Linear models in particular assume that features are independent of each other, and if features are strongly correlated, that could introduce bias into our model.\n",
    "\n",
    "Let's take a look at the wine dataset again, which is made up of continuous, numerical features. We run Pearson's correlation coefficient on the dataset to determine which columns are good candidates for eliminating. Then, remove those columns from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.516995Z",
     "start_time": "2020-02-16T17:50:54.488042Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the column correlations of the wine dataset\n",
    "corr_matrix = utils.wine.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.546918Z",
     "start_time": "2020-02-16T17:50:54.534938Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a minute to find the column where the correlation value is greater than 0.75 at least twice or run the following code\n",
    "corrs = corr_matrix.abs().unstack().sort_values(kind='quicksort', ascending=False)\n",
    "corrs[(corrs>0.75) & (corrs<1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.578800Z",
     "start_time": "2020-02-16T17:50:54.571849Z"
    }
   },
   "outputs": [],
   "source": [
    "# Flavanoids has high correlation with Total phenols and OD280/OD315 of diluted wines\n",
    "# Proline is already redundant because of Proline_log\n",
    "# We don't drop Type because it the target column\n",
    "to_drop = [\"Flavanoids\", \"Proline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.625675Z",
     "start_time": "2020-02-16T17:50:54.608726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop that column from the DataFrame\n",
    "utils.wine = utils.wine.drop(to_drop, axis=1)\n",
    "utils.wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.672582Z",
     "start_time": "2020-02-16T17:50:54.648613Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.wine.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Selecting features using text vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we used scikit-learn to create a tf-idf vector of text in one of our datasets. We don't necessarily need the entire vector to train a model, though. We could potentially take something like the top 20% of weighted words across the vector. This is a scenario where iteration is important, and it might be helpful to test out different subsets of our tf-idf vector to see what works.\n",
    "\n",
    "Rather than just blindly taking some top percentage of a tf-idf vector, let's look at how to pull out the words and their weights on a per document basis. It isn't especially straightforward to do this in scikit-learn, but it's very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring text vectors, part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's expand on the text vector exploration method we just learned about, using the `volunteer` dataset's title `tf/idf` vectors. We'll return a list of numbers with the function. In the next example, we'll write another function to collect the top words across all documents, extract them, and then use that list to filter down our `text_tfidf` vector.\n",
    "\n",
    "We'll pass in the reversed vocab list, the vector, and the row we want to retrieve data for. We'll do row zipping to a dictionary in the function. And finally, we'll return a dictionary mapping the word to its score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.704496Z",
     "start_time": "2020-02-16T17:50:54.690524Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add in the rest of the parameters\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Let's transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Let's sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.751338Z",
     "start_time": "2020-02-16T17:50:54.733388Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the weighted words\n",
    "# It'll be easier later on if we have the index number in the key position in the dictionary\n",
    "vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
    "return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we could sort by score, or eliminate words below a certain threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring text vectors, part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function we wrote in the previous example, we're going to extract the top words from each document in the text vector, return a list of the word indices, and use that list to filter the text vector down to those top words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.782281Z",
     "start_time": "2020-02-16T17:50:54.775300Z"
    }
   },
   "outputs": [],
   "source": [
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Here we'll call the function from the previous example, and extend the list we're creating\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.438565Z",
     "start_time": "2020-02-16T17:50:54.813205Z"
    }
   },
   "outputs": [],
   "source": [
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Naive Bayes with feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-run the Naive Bayes text classification model from earlier, with our selection choices from the previous example, on the `volunteer` dataset's `title` and `category_desc` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.454521Z",
     "start_time": "2020-02-16T17:50:55.440541Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = utils.volunteer[\"category_desc\"]\n",
    "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.486408Z",
     "start_time": "2020-02-16T17:50:55.456488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "nb.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.516365Z",
     "start_time": "2020-02-16T17:50:55.488370Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the model's accuracy\n",
    "nb.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our accuracy score wasn't that different from our previous. That's okay; the title field is a very small text field, appropriate for demonstrating how filtering vectors works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A less manual way of reducing the size of our feature set is through dimensionality reduction.\n",
    "\n",
    "Dimensionality reduction is a form of unsupervised learning that transforms our data in a way that shrinks the number of features in our feature space. This data transformation can be done in a linear or nonlinear fashion. Dimensionality reduction is a feature extraction method, given that data is being transformed into new and different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA uses a linear transformation to project features into a space where they are completely uncorrelated. While the feature space is reduced, the variance is captured in a meaningful way by combining features into components. PCA captures, in each component, as much of the variance in the dataset as possible. In terms of feature selection, it can be a useful method when we have a large number of features and no strong candidates for elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply PCA to the wine dataset, to see if we can get an increase in our model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.531353Z",
     "start_time": "2020-02-16T17:50:55.517331Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.547307Z",
     "start_time": "2020-02-16T17:50:55.533324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up PCA and the X vector for diminsionality reduction\n",
    "pca = PCA()\n",
    "wine_X = utils.wine.drop(\"Type\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.562243Z",
     "start_time": "2020-02-16T17:50:55.548303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply PCA to the wine dataset X vector\n",
    "transformed_X = pca.fit_transform(wine_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.577234Z",
     "start_time": "2020-02-16T17:50:55.566235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the percentage of variance explained by the different components\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA caveats\n",
    "\n",
    "* difficult to interpret PCA components beyond which components explain the most variance\n",
    "* PCA is a good step to do at the end of our preprocessing journey, because of the way the data gets transformed and reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we'll train a model using the PCA-transformed vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have run PCA on the `wine` dataset, let's try training a model with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.592191Z",
     "start_time": "2020-02-16T17:50:55.580198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the transformed X and the y labels into training and test sets\n",
    "wine_y = utils.wine[\"Type\"]\n",
    "X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X, wine_y, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.608120Z",
     "start_time": "2020-02-16T17:50:55.595153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit knn to the training data\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_wine_train, y_wine_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.623080Z",
     "start_time": "2020-02-16T17:50:55.611110Z"
    }
   },
   "outputs": [],
   "source": [
    "# Score knn on the test data and print it out\n",
    "knn.score(X_wine_test, y_wine_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned all about preprocessing we'll try these techniques out on a dataset that records information on UFO sightings. \n",
    "\n",
    "Each row in this dataset contains information like the location, the type of the sighting, the number of seconds and minutes the sighting lasted, a description of the sighting, and the date the sighting was recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## UFOs and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking column types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the UFO dataset's column types using the `dtypes` attribute. One column jumps out for transformation: the `date` column, which can be transformed into the `datetime` type. That will make our feature engineering efforts easier later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.669462Z",
     "start_time": "2020-02-16T17:50:55.626070Z"
    }
   },
   "outputs": [],
   "source": [
    "ufo = pd.read_csv('data/ufo_sightings_large.csv')\n",
    "ufo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.685420Z",
     "start_time": "2020-02-16T17:50:55.671459Z"
    }
   },
   "outputs": [],
   "source": [
    "ufo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.091209Z",
     "start_time": "2020-02-16T17:50:55.687414Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change the date column to type datetime\n",
    "ufo[\"date\"] = pd.to_datetime(ufo[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove some of the rows where certain columns have missing values. We're going to look at the `length_of_time column`, the `state` column, and the `type` column. If any of the values in these columns are missing, we're going to drop the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.107200Z",
     "start_time": "2020-02-16T17:50:56.093204Z"
    }
   },
   "outputs": [],
   "source": [
    "ufo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.122162Z",
     "start_time": "2020-02-16T17:50:56.109163Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check how many values are missing in the length_of_time, state, and type columns\n",
    "ufo[['length_of_time', 'state', 'type']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.138110Z",
     "start_time": "2020-02-16T17:50:56.124146Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep only rows where length_of_time, state, and type are not null\n",
    "ufo = ufo[ufo[\"length_of_time\"].notnull() & ufo[\"state\"].notnull() & ufo[\"type\"].notnull()]\n",
    "ufo.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.154063Z",
     "start_time": "2020-02-16T17:50:56.141076Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the shape of the new dataset\n",
    "ufo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Categorical variables and standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of categorical variables in the UFO dataset, including location data and the type of the encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting numbers from strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `length_of_time` field in the UFO dataset is a text field that has the number of minutes within the string. Here, we'll extract that number from that text field using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.170027Z",
     "start_time": "2020-02-16T17:50:56.158033Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def return_minutes(time_string):\n",
    "    # We'll use \\d+ to grab digits and match it to the column values\n",
    "    pattern = re.compile(r\"(\\d+|\\d{1,2}\\.\\d{1,2})(?:[^0-9\\.]*)?(?:minutes*|mins*)|(\\d+):(?:\\d*?)\")    \n",
    "        \n",
    "    # Use match on the pattern and column\n",
    "    num = re.search(pattern, time_string)\n",
    "    if num is not None:\n",
    "        return math.floor(float((num.group(1) if num.group(1) is not None else num.group(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.199953Z",
     "start_time": "2020-02-16T17:50:56.171995Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the extraction to the length_of_time column\n",
    "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.215879Z",
     "start_time": "2020-02-16T17:50:56.201915Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the head of both of the columns\n",
    "ufo[[\"length_of_time\", \"minutes\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we end up with some `NaN`s in the DataFrame. That's okay for now; we'll take care of those before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.231858Z",
     "start_time": "2020-02-16T17:50:56.217876Z"
    }
   },
   "outputs": [],
   "source": [
    "ufo = ufo[(ufo['seconds'] != 0.0) & ufo['minutes'].notnull()]\n",
    "ufo.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying features for standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll investigate the variance of columns in the UFO dataset to determine which features should be standardized. After taking a look at the variances of the `seconds` and `minutes` column, we'll see that the variance of the seconds column is extremely high. Because `seconds` and `minutes` are related to each other (an issue we'll deal with when we select features for modeling), let's log normlize the `seconds` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.263782Z",
     "start_time": "2020-02-16T17:50:56.233830Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the variance of the columns\n",
    "ufo.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.279738Z",
     "start_time": "2020-02-16T17:50:56.265748Z"
    }
   },
   "outputs": [],
   "source": [
    "# Log normalize the seconds column\n",
    "ufo[\"seconds_log\"] = np.log(ufo[\"seconds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.295667Z",
     "start_time": "2020-02-16T17:50:56.281702Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the variance of just the seconds_log column\n",
    "print(ufo[\"seconds_log\"].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Engineering new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are couple of columns in the UFO dataset that need to be encoded before they can be modeled through scikit-learn. We'll do that transformation here, using both binary and one-hot encoding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.310663Z",
     "start_time": "2020-02-16T17:50:56.296695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use Pandas to encode us values as 1 and others as 0\n",
    "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda val: 1 if val == \"us\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.326581Z",
     "start_time": "2020-02-16T17:50:56.312619Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the number of unique type values\n",
    "print(len(ufo[\"type\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.341574Z",
     "start_time": "2020-02-16T17:50:56.329575Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.357523Z",
     "start_time": "2020-02-16T17:50:56.344532Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo = pd.concat([ufo, type_set], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.389447Z",
     "start_time": "2020-02-16T17:50:56.359495Z"
    }
   },
   "outputs": [],
   "source": [
    "ufo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features from dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature engineering task to perform is month and year extraction. We'll perform this task on the `date` column of the `ufo` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.405397Z",
     "start_time": "2020-02-16T17:50:56.391427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the first 5 rows of the date column\n",
    "ufo[\"date\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.421360Z",
     "start_time": "2020-02-16T17:50:56.407367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the month from the date column\n",
    "ufo[\"month\"] = ufo[\"date\"].apply(lambda row: row.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.437286Z",
     "start_time": "2020-02-16T17:50:56.426317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the year from the date column\n",
    "ufo[\"year\"] = ufo[\"date\"].apply(lambda row: row.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.453275Z",
     "start_time": "2020-02-16T17:50:56.440278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the head of all three columns\n",
    "ufo[[\"date\", \"month\", \"year\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the `desc` column in the UFO dataset into tf/idf vectors, since there's likely something we can learn from this field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.469225Z",
     "start_time": "2020-02-16T17:50:56.456234Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the head of the desc field\n",
    "ufo[\"desc\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.485157Z",
     "start_time": "2020-02-16T17:50:56.471195Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the tfidf vectorizer object\n",
    "vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.532060Z",
     "start_time": "2020-02-16T17:50:56.487151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use vec's fit_transform method on the desc field\n",
    "desc_tfidf = vec.fit_transform(ufo[\"desc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.547017Z",
     "start_time": "2020-02-16T17:50:56.533031Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the number of columns this creates.\n",
    "desc_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text vector has a large number of columns. We'll work on selecting the features we want to use for modeling in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Feature selection and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the ideal dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of some of the unnecessary features. Because we have an encoded country column, `country_enc`, we keep it and drop other columns related to location: `city`, `country`, `lat`, `long`, `state`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have columns related to `month` and `year`, so we don't need the `date` or `recorded` columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vectorized `desc`, so we don't need it anymore. For now we'll keep `type`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll keep `seconds_log` and drop `seconds` and `minutes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get rid of the `length_of_time` column, which is unnecessary after extracting `minutes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.561977Z",
     "start_time": "2020-02-16T17:50:56.548987Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the correlation between the seconds, seconds_log, and minutes columns\n",
    "ufo[[\"seconds\", \"seconds_log\", \"minutes\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.577936Z",
     "start_time": "2020-02-16T17:50:56.563947Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a list of features to drop   \n",
    "to_drop = [\"city\", \"country\", \"date\", \"desc\", \"lat\", \"length_of_time\", \"long\", \"minutes\", \"recorded\", \"seconds\", \"state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.592896Z",
     "start_time": "2020-02-16T17:50:56.580905Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop those features\n",
    "ufo = ufo.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:58.966940Z",
     "start_time": "2020-02-16T17:50:56.604840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's also filter some words out of the text vector we created\n",
    "vocab = {v:k for k,v in vec.vocabulary_.items()}\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost done. In the next examples, we'll try modeling the UFO data in a couple of different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling the UFO dataset, part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're going to build a k-nearest neighbor model to predict which country the UFO sighting took place in. Our `X` dataset has the log-normalized seconds column, the one-hot encoded type columns, as well as the month and year when the sighting took place. The `y` labels are the encoded country column, where 1 is `us` and 0 is `ca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:58.982898Z",
     "start_time": "2020-02-16T17:50:58.969932Z"
    }
   },
   "outputs": [],
   "source": [
    "X = ufo.drop(['type', 'country_enc'], axis=1)\n",
    "y = ufo['country_enc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:58.997882Z",
     "start_time": "2020-02-16T17:50:58.985888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the features in the X set of data\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:59.013846Z",
     "start_time": "2020-02-16T17:50:59.000848Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the X and y sets using train_test_split, setting stratify=y\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:59.043760Z",
     "start_time": "2020-02-16T17:50:59.016821Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit knn to the training sets\n",
    "knn.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:59.074661Z",
     "start_time": "2020-02-16T17:50:59.045742Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the score of knn on the test sets\n",
    "print(knn.score(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs pretty well. It seems like we've made pretty good feature selection choices here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling the UFO dataset, part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's build a model using the text vector we created, `desc_tfidf`, using the `filtered_words` list to create a filtered text vector. Let's see if we can predict the `type` of the sighting based on the text. We'll use a Naive Bayes model for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:59.089637Z",
     "start_time": "2020-02-16T17:50:59.077644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the list of filtered words we created to filter the text vector\n",
    "filtered_text = desc_tfidf[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:59.183039Z",
     "start_time": "2020-02-16T17:50:59.093603Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the X and y sets using train_test_split, setting stratify=y\n",
    "y = ufo['type']\n",
    "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:59.336664Z",
     "start_time": "2020-02-16T17:50:59.185032Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit nb to the training sets\n",
    "nb.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:51:00.037493Z",
     "start_time": "2020-02-16T17:50:59.338624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the score of nb on the test sets\n",
    "nb.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this model performs very poorly on this text data. This is a clear case where iteration would be necessary to figure out what subset of text improves the model, and if perhaps any of the other features are useful in predicting `type`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know how to deal with basic data issues like missing data and incorrect types, how to standardize numerical values and process categorical ones, how to engineer new features that will improve our dataset, and finally, how to select features for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**[Week 5 - Data Preprocessing and Hyperparameter Tuning](https://radu-enuca.gitbook.io/ml-challenge/preprocessing-and-tuning)**\n",
    "\n",
    "*Have questions or comments? Visit the ML Challenge Mattermost Channel.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "393.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
