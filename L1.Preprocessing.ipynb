{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/ML-Challenge/week5-preprocessing-and-tunning/blob/master/L1.Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lesson covers the basics of how and when to perform data preprocessing. This essential step in any machine learning project is when we get our data ready for modeling. Between importing and cleaning the data and fitting the machine learning model is when preprocessing comes into play. We'll learn how to standardize the data so that it's in the right form for the model, create new features to best leverage the information in the dataset, and select the best features to improve the model fit. Finally, we'll have some practice preprocessing by getting a dataset on UFO sightings ready for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.017074Z",
     "start_time": "2020-02-16T17:50:51.009105Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Download lesson datasets\n",
    "# Required if you're using Google Colab\n",
    "#!wget \"https://github.com/ML-Challenge/week5-preprocessing-and-tunning/raw/master/datasets.zip\"\n",
    "#!unzip -o datasets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.541783Z",
     "start_time": "2020-02-16T17:50:51.048543Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import utils\n",
    "# We'll be using this module throughout the lesson\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.759200Z",
     "start_time": "2020-02-16T17:50:51.544741Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# and setting the size of all plots.\n",
    "plt.rcParams['figure.figsize'] = [11, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll learn exactly what it means to `preprocess` data. We'll take the first steps in any preprocessing journey, including exploring data types and dealing with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is data preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data - columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset comprised of volunteer information from New York City. The dataset has a number of features, but we want to get rid of features that have at least 3 missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many features are in the original dataset, and how many features are in the set after columns with at least 3 missing values are removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.790117Z",
     "start_time": "2020-02-16T17:50:51.761197Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.821037Z",
     "start_time": "2020-02-16T17:50:51.792110Z"
    }
   },
   "outputs": [],
   "source": [
    "volunteer_clean = utils.volunteer.dropna(axis=1, thresh=3)\n",
    "volunteer_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data - rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the `volunteer` dataset again, we want to drop rows where the `category_desc` column values are missing. We're going to do this using boolean indexing, by checking to see if we have any null values, and then filtering the dataset so that we only have rows with those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.835989Z",
     "start_time": "2020-02-16T17:50:51.823031Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check how many values are missing in the category_desc column\n",
    "print(utils.volunteer['category_desc'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.850949Z",
     "start_time": "2020-02-16T17:50:51.837958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Subset the volunteer dataset\n",
    "volunteer_subset = utils.volunteer[utils.volunteer['category_desc'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.865912Z",
     "start_time": "2020-02-16T17:50:51.851946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the shape of the subset\n",
    "print(volunteer_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: we can use boolean indexing to effectively subset DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking another look at the dataset comprised of volunteer information from New York City, we want to know what types we'll be working with as we start to do more preprocessing. Which data types are present in the volunteer dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.881871Z",
     "start_time": "2020-02-16T17:50:51.868876Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have int, float and object (string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting a column type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the `volunteer` dataset types, we'll see that the column `hits` is type object. But, if we actually look at the column, we'll see that it consists of integers. Let's convert that column to type int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.896826Z",
     "start_time": "2020-02-16T17:50:51.883836Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the head of the hits column\n",
    "print(utils.volunteer[\"hits\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use astype to convert between a variety of types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.912783Z",
     "start_time": "2020-02-16T17:50:51.898796Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the hits column to type int\n",
    "utils.volunteer[\"hits\"] = utils.volunteer[\"hits\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.927751Z",
     "start_time": "2020-02-16T17:50:51.914758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the dtypes of the dataset\n",
    "print(utils.volunteer.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `volunteer` dataset, we're thinking about trying to predict the `category_desc` variable using the other features in the dataset. First, though, we need to know what the class distribution (and imbalance) is for that label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which descriptions occur less than 50 times in the volunteer dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.942710Z",
     "start_time": "2020-02-16T17:50:51.928747Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer['category_desc'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Emergency Prepardness and Environment occur less than 50 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the distribution of variables in the `category_desc` column in the `volunteer` dataset is uneven. If we wanted to train a model to try to predict `category_desc`, we would want to train the model on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.958661Z",
     "start_time": "2020-02-16T17:50:51.944677Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a data with all columns except category_desc\n",
    "volunteer_X = utils.volunteer.drop('category_desc', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:51.974627Z",
     "start_time": "2020-02-16T17:50:51.960631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a category_desc labels dataset\n",
    "volunteer_y = utils.volunteer[['category_desc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.409469Z",
     "start_time": "2020-02-16T17:50:51.976590Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use stratified sampling to split up the dataset according to the volunteer_y dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.425390Z",
     "start_time": "2020-02-16T17:50:52.411429Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the category_desc counts on the training y labels\n",
    "print(y_train['category_desc'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the lesson is all about standardizing data. Often a model will make some assumptions about the distribution or scale of the features. Standardization is a way to make the data fit these assumptions and improve the algorithm's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is data standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to standardize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned when it is appropriate to standardize our data, which of these scenarios would we **NOT** want to standardize?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "1. A column we want to use for modeling has extremely high variance.\n",
    "2. We have a dataset with several continuous columns on different scales and we'd like to use a linear model to train the data.\n",
    "3. The models we're working with use some sort of distance metric in a linear space, like the Euclidean metric.\n",
    "4. Our dataset is comprised of categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.441348Z",
     "start_time": "2020-02-16T17:50:52.427386Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use 1,2,3 or 4 as a parameter\n",
    "utils.when_to_standardize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling without normalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what might happen to our model's accuracy if we try to model data without doing some sort of standardization first. Here we have a subset of the `wine` dataset. One of the columns, `Proline`, has an extremely high variance compared to the other columns. This is an example of where a technique like log normalization would come in handy, which we'll learn about later in this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.473305Z",
     "start_time": "2020-02-16T17:50:52.443344Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.489253Z",
     "start_time": "2020-02-16T17:50:52.476255Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a subset of data\n",
    "wine_X = utils.wine[['Proline', 'Total phenols', 'Hue', 'Nonflavanoid phenols']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.505213Z",
     "start_time": "2020-02-16T17:50:52.492212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Type labels dataset\n",
    "wine_y = utils.wine['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.520161Z",
     "start_time": "2020-02-16T17:50:52.507206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_X, wine_y, stratify=wine_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.581002Z",
     "start_time": "2020-02-16T17:50:52.522132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the k-nearest neighbors model to the training data\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.596966Z",
     "start_time": "2020-02-16T17:50:52.582971Z"
    }
   },
   "outputs": [],
   "source": [
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy score is pretty low. Let's explore methods to improve this score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the variance of the columns in the wine dataset and see which column is a candidate for normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.612914Z",
     "start_time": "2020-02-16T17:50:52.598927Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.wine.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `Proline` column has an extremely high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log normalization in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that the `Proline` column in our wine dataset has a large amount of variance, let's log normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.627876Z",
     "start_time": "2020-02-16T17:50:52.614889Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the variance of the Proline column\n",
    "print(utils.wine['Proline'].var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.642835Z",
     "start_time": "2020-02-16T17:50:52.629870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the log normalization function to the Proline column\n",
    "utils.wine['Proline_log'] = np.log(utils.wine['Proline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.657783Z",
     "start_time": "2020-02-16T17:50:52.645803Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the variance of the normalized Proline column\n",
    "print(utils.wine['Proline_log'].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `np.log()` function is an easy way to log normalize a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.673754Z",
     "start_time": "2020-02-16T17:50:52.659765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a subset of data\n",
    "wine_X = utils.wine[['Proline_log', 'Total phenols', 'Hue', 'Nonflavanoid phenols']]\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_X, wine_y, stratify=wine_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.689721Z",
     "start_time": "2020-02-16T17:50:52.675743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.704670Z",
     "start_time": "2020-02-16T17:50:52.691681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling data for feature comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data - investigating columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to use the `Ash`, `Alcalinity of ash`, and `Magnesium` columns in the wine dataset to train a linear model, but it's possible that these columns are all measured in different ways, which would bias a linear model. Using `describe()` to return descriptive statistics about this dataset, which of the following statements are true about the scale of data in these columns?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "1. The max of `Ash` is 3.23, the max of `Alcalinity of ash` is 30, and the max of `Magnesium` is 162.\n",
    "2. The means of `Ash` and `Alcalinity of ash` are less than 20, while the mean of `Magnesium` is greater than 90.\n",
    "3. The standard deviations of `Ash` and `Alcalinity of ash` are equal.\n",
    "4. 1 and 2 are true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.767509Z",
     "start_time": "2020-02-16T17:50:52.707638Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.wine.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data - standardizing columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know that the `Ash`, `Alcalinity of ash`, and `Magnesium` columns in the wine dataset are all on different scales, let's standardize them in a way that allows for use in a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.783466Z",
     "start_time": "2020-02-16T17:50:52.769505Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import StandardScaler from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.798395Z",
     "start_time": "2020-02-16T17:50:52.785463Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the scaler\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.818421Z",
     "start_time": "2020-02-16T17:50:52.802385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a subset of the DataFrame we want to scale \n",
    "wine_subset = utils.wine[['Ash', 'Alcalinity of ash', 'Magnesium']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.833914Z",
     "start_time": "2020-02-16T17:50:52.820924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the scaler to the DataFrame subset\n",
    "wine_subset_scaled = ss.fit_transform(wine_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.865834Z",
     "start_time": "2020-02-16T17:50:52.835887Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(wine_subset_scaled, columns=['Ash', 'Alcalinity of ash', 'Magnesium']).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, running `fit_transform` during preprocessing will both fit the method to the data as well as transform the data in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized data and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN on non-scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at the accuracy of a K-nearest neighbors model on the `wine` dataset without standardizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.881784Z",
     "start_time": "2020-02-16T17:50:52.867799Z"
    }
   },
   "outputs": [],
   "source": [
    "X = utils.wine.drop(['Type', 'Proline_log'], axis=1)\n",
    "y = utils.wine['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.897755Z",
     "start_time": "2020-02-16T17:50:52.883784Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.913708Z",
     "start_time": "2020-02-16T17:50:52.898747Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.928669Z",
     "start_time": "2020-02-16T17:50:52.915706Z"
    }
   },
   "outputs": [],
   "source": [
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN on scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score on the unscaled `wine` dataset was decent, but we can likely do better if we scale the dataset. The process is mostly the same as the previous example, with the added step of scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.944629Z",
     "start_time": "2020-02-16T17:50:52.931630Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the scaling method.\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.960576Z",
     "start_time": "2020-02-16T17:50:52.946620Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the scaling method to the dataset used for modeling.\n",
    "X_scaled = ss.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.975514Z",
     "start_time": "2020-02-16T17:50:52.962546Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:52.991467Z",
     "start_time": "2020-02-16T17:50:52.977504Z"
    }
   },
   "outputs": [],
   "source": [
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The increase in accuracy is worth the extra step of scaling the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll learn about feature engineering. We'll explore different ways to create new, more useful, features from the ones already in the dataset. We'll see how to encode, aggregate, and extract information from both numerical and textual features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering knowledge test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned about feature engineering, which of the following examples are good candidates for creating new features?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "1. A column of timestamps\n",
    "2. A column of newspaper headlines\n",
    "3. A column of weight measurements\n",
    "4. 1 and 2\n",
    "5. None of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.022385Z",
     "start_time": "2020-02-16T17:50:53.016403Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use 1,2,3,4 or 5 as parameter\n",
    "utils.feature_engineering_puzzle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying areas for feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an exploratory look at the `volunteer` dataset. Which of the following columns would we want to perform a feature engineering task on?\n",
    "\n",
    "**Posible Answers**\n",
    "\n",
    "1. `vol_requests`\n",
    "2. `title`\n",
    "3. `created_date`\n",
    "4. `category_desc`\n",
    "5. 2,3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.099212Z",
     "start_time": "2020-02-16T17:50:53.067267Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.115142Z",
     "start_time": "2020-02-16T17:50:53.110187Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use 1,2,3,4 or 5 as parameter\n",
    "utils.identity_features_puzzle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables - binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the `hiking` dataset. There are several columns here that need encoding, one of which is the `Accessible` column, which needs to be encoded in order to be modeled. `Accessible` is a binary feature, so it has two values - either `Y` or `N` - so it needs to be encoded into 1s and 0s. We'll use scikit-learn's `LabelEncoder` method to do that transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.176974Z",
     "start_time": "2020-02-16T17:50:53.152073Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.hiking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.207913Z",
     "start_time": "2020-02-16T17:50:53.195923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.237842Z",
     "start_time": "2020-02-16T17:50:53.230890Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.284720Z",
     "start_time": "2020-02-16T17:50:53.264771Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the encoding to the \"Accessible\" column\n",
    "utils.hiking['Accessible_enc'] = enc.fit_transform(utils.hiking['Accessible'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.331627Z",
     "start_time": "2020-02-16T17:50:53.305695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare the two columns\n",
    "utils.hiking[['Accessible', 'Accessible_enc']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! `.fit_transform()` is a good way to both fit an encoding and transform the data in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables - one-hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the columns in the volunteer dataset, `category_desc`, gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically. We'll use Pandas' `get_dummies()` function to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.362510Z",
     "start_time": "2020-02-16T17:50:53.345589Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(utils.volunteer['category_desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.408386Z",
     "start_time": "2020-02-16T17:50:53.393459Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the encoded columns\n",
    "category_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_dummies()` is a simple and quick way to encode categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering numerical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering numerical features - taking an average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good use case for taking an aggregate statistic to create a new feature is to take the mean of columns. Here, we have a DataFrame of running times named `running_times_5k`. For each `name` in the dataset, take the mean of their 5 run times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.456261Z",
     "start_time": "2020-02-16T17:50:53.440332Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.running_times_5k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.502137Z",
     "start_time": "2020-02-16T17:50:53.486212Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of the columns to average\n",
    "run_columns = ['run1', 'run2', 'run3', 'run4', 'run5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.547024Z",
     "start_time": "2020-02-16T17:50:53.522130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use apply to create a mean column\n",
    "utils.running_times_5k[\"mean\"] = utils.running_times_5k.apply(lambda row: row[run_columns].mean(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.591904Z",
     "start_time": "2020-02-16T17:50:53.564984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the results\n",
    "utils.running_times_5k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambdas are especially helpful for operating across columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering numerical features - datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several columns in the `volunteer` dataset comprised of datetimes. Let's take a look at the `start_date_date` column and extract just the month to use as a feature for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.623315Z",
     "start_time": "2020-02-16T17:50:53.605901Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer[['start_date_date', 'end_date_date']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.717614Z",
     "start_time": "2020-02-16T17:50:53.646232Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, convert string column to date column\n",
    "utils.volunteer[\"start_date_converted\"] = pd.to_datetime(utils.volunteer[\"start_date_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.733573Z",
     "start_time": "2020-02-16T17:50:53.720576Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer[['start_date_date', 'start_date_converted']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.749530Z",
     "start_time": "2020-02-16T17:50:53.736534Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract just the month from the converted column\n",
    "utils.volunteer['start_date_month'] = utils.volunteer['start_date_converted'].apply(lambda row: row.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.781445Z",
     "start_time": "2020-02-16T17:50:53.767475Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the converted and new month columns\n",
    "utils.volunteer[['start_date_date', 'start_date_converted', 'start_date_month']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use attributes like `.day` to get the day and `.year` to get the year from datetime columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering features from strings - extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Length` column in the `hiking` dataset is a column of strings, but contained in the column is the mileage for the hike. We're going to extract this mileage using regular expressions, and then use a lambda in Pandas to apply the extraction to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.828802Z",
     "start_time": "2020-02-16T17:50:53.808390Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.hiking.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.860740Z",
     "start_time": "2020-02-16T17:50:53.849745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write a pattern to extract numbers and decimals\n",
    "import re\n",
    "\n",
    "def return_mileage(length):\n",
    "    if length is not None:\n",
    "        pattern = re.compile(r\"\\d+\\.\\d+\")\n",
    "\n",
    "        # Search the text for matches\n",
    "        mile = re.match(pattern, length)\n",
    "\n",
    "        # If a value is returned, use group(0) to return the found value\n",
    "        if mile is not None:\n",
    "            return float(mile.group(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.892629Z",
     "start_time": "2020-02-16T17:50:53.884668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the function to the Length column\n",
    "utils.hiking[\"Length_num\"] = utils.hiking[\"Length\"].apply(lambda row: return_mileage(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.938532Z",
     "start_time": "2020-02-16T17:50:53.923582Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at both columns\n",
    "utils.hiking[[\"Length\", \"Length_num\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering features from strings - tf/idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the `volunteer` dataset's `title` column into a text vector, to use in a prediction task in the next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:53.969457Z",
     "start_time": "2020-02-16T17:50:53.962470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take the title text\n",
    "title_text = utils.volunteer.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.016326Z",
     "start_time": "2020-02-16T17:50:54.001341Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the vectorizer method\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.047251Z",
     "start_time": "2020-02-16T17:50:54.036279Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the text into tf-idf vectors\n",
    "text_tfidf = tfidf_vec.fit_transform(title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using tf/idf vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've encoded the`volunteer` dataset's `title` column into tf/idf vectors, let's use those vectors to try to predict the `category_desc` column. Notice that we have to run the `toarray()` method on the tf/idf vector, in order to get in it the proper format for scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.094127Z",
     "start_time": "2020-02-16T17:50:54.076165Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = utils.volunteer[\"category_desc\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.140003Z",
     "start_time": "2020-02-16T17:50:54.120024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's use Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.187875Z",
     "start_time": "2020-02-16T17:50:54.160916Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the model's accuracy\n",
    "nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model doesn't score very well. We'll work on selecting the best features for modeling in the next part of the lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Selecting features for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section goes over a few different techniques for selecting the most important features from the dataset. We'll learn how to drop redundant features, work with text vectors, and reduce the number of features in the dataset using principal component analysis (PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we had finished standardizing our data and creating new features. Which of the following scenarios is **NOT** a good candidate for feature selection?\n",
    "\n",
    "**Possible Answers**\n",
    "\n",
    "1. Several columns of running times that have been averaged into a new column.\n",
    "2. A text field that hasn't been turned into a tf/idf vector yet.\n",
    "3. A column of text that has already had a float extracted out of it.\n",
    "4. A categorical field that has been one-hot encoded.\n",
    "5. The dataset contains columns related to whether something is a fruit or vegetable, the name of the fruit or vegetable, and the scientific name of the plant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.219792Z",
     "start_time": "2020-02-16T17:50:54.205829Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use 1,2,3,4 or 5 as parameter\n",
    "utils.feature_selction_puzzle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing redundant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting relevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's identify the redundant columns in the `volunteer_processed` dataset and perform feature selection on the dataset to return a DataFrame of the relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we explore the volunteer dataset, we'll see three features which are related to location: `locality`, `region`, and `postalcode`. They contain repeated information, so it would make sense to keep only one of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.267662Z",
     "start_time": "2020-02-16T17:50:54.250707Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer[['locality', 'region', 'postalcode']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also features that have gone through the feature engineering process: columns like 'Education' and 'Emergency Preparedness' are a product of encoding the categorical variable `category_desc`, so `category_desc` itself is redundant now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.313540Z",
     "start_time": "2020-02-16T17:50:54.297575Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.345422Z",
     "start_time": "2020-02-16T17:50:54.335477Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.volunteer_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.376340Z",
     "start_time": "2020-02-16T17:50:54.370385Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of redundant column names to drop\n",
    "to_drop = [\"category_desc\", \"created_date\", \"locality\", \"region\", \"vol_requests\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.423239Z",
     "start_time": "2020-02-16T17:50:54.403299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop those columns from the dataset\n",
    "volunteer_subset = utils.volunteer_processed.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.471120Z",
     "start_time": "2020-02-16T17:50:54.447184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the head of the new dataset\n",
    "volunteer_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for correlated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the wine dataset again, which is made up of continuous, numerical features. We run Pearson's correlation coefficient on the dataset to determine which columns are good candidates for eliminating. Then, remove those columns from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.516995Z",
     "start_time": "2020-02-16T17:50:54.488042Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the column correlations of the wine dataset\n",
    "corr_matrix = utils.wine.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.546918Z",
     "start_time": "2020-02-16T17:50:54.534938Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a minute to find the column where the correlation value is greater than 0.75 at least twice or run the following code\n",
    "corrs = corr_matrix.abs().unstack().sort_values(kind='quicksort', ascending=False)\n",
    "corrs[(corrs>0.75) & (corrs<1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.578800Z",
     "start_time": "2020-02-16T17:50:54.571849Z"
    }
   },
   "outputs": [],
   "source": [
    "# Flavanoids has high correlation with Total phenols and OD280/OD315 of diluted wines\n",
    "# Proline is already redundant because of Proline_log\n",
    "# We don't drop Type because it the target column\n",
    "to_drop = [\"Flavanoids\", \"Proline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.625675Z",
     "start_time": "2020-02-16T17:50:54.608726Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop that column from the DataFrame\n",
    "utils.wine = utils.wine.drop(to_drop, axis=1)\n",
    "utils.wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.672582Z",
     "start_time": "2020-02-16T17:50:54.648613Z"
    }
   },
   "outputs": [],
   "source": [
    "utils.wine.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features using text vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring text vectors, part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's expand on the text vector exploration method we just learned about, using the `volunteer` dataset's title tf/idf vectors. In this first part of text vector exploration, we're going to add to that function we learned about above. We'll return a list of numbers with the function. In the next example, we'll write another function to collect the top words across all documents, extract them, and then use that list to filter down our `text_tfidf` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.704496Z",
     "start_time": "2020-02-16T17:50:54.690524Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add in the rest of the parameters\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Let's transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Let's sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.751338Z",
     "start_time": "2020-02-16T17:50:54.733388Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the weighted words\n",
    "vocab = {v:k for k,v in tfidf_vec.vocabulary_.items()}\n",
    "return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring text vectors, part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function we wrote in the previous example, we're going to extract the top words from each document in the text vector, return a list of the word indices, and use that list to filter the text vector down to those top words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:54.782281Z",
     "start_time": "2020-02-16T17:50:54.775300Z"
    }
   },
   "outputs": [],
   "source": [
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Here we'll call the function from the previous example, and extend the list we're creating\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.438565Z",
     "start_time": "2020-02-16T17:50:54.813205Z"
    }
   },
   "outputs": [],
   "source": [
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Naive Bayes with feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-run the Naive Bayes text classification model from earlier, with our selection choices from the previous example, on the `volunteer` dataset's `title` and `category_desc` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.454521Z",
     "start_time": "2020-02-16T17:50:55.440541Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the dataset according to the class distribution of category_desc\n",
    "y = utils.volunteer[\"category_desc\"]\n",
    "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.486408Z",
     "start_time": "2020-02-16T17:50:55.456488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "nb.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.516365Z",
     "start_time": "2020-02-16T17:50:55.488370Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the model's accuracy\n",
    "nb.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our accuracy score wasn't that different from our previous. That's okay; the title field is a very small text field, appropriate for demonstrating how filtering vectors works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply PCA to the wine dataset, to see if we can get an increase in our model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.531353Z",
     "start_time": "2020-02-16T17:50:55.517331Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.547307Z",
     "start_time": "2020-02-16T17:50:55.533324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up PCA and the X vector for diminsionality reduction\n",
    "pca = PCA()\n",
    "wine_X = utils.wine.drop(\"Type\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.562243Z",
     "start_time": "2020-02-16T17:50:55.548303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply PCA to the wine dataset X vector\n",
    "transformed_X = pca.fit_transform(wine_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.577234Z",
     "start_time": "2020-02-16T17:50:55.566235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the percentage of variance explained by the different components\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we'll train a model using the PCA-transformed vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have run PCA on the `wine` dataset, let's try training a model with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.592191Z",
     "start_time": "2020-02-16T17:50:55.580198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the transformed X and the y labels into training and test sets\n",
    "wine_y = utils.wine[\"Type\"]\n",
    "X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(transformed_X, wine_y, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.608120Z",
     "start_time": "2020-02-16T17:50:55.595153Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit knn to the training data\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_wine_train, y_wine_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.623080Z",
     "start_time": "2020-02-16T17:50:55.611110Z"
    }
   },
   "outputs": [],
   "source": [
    "# Score knn on the test data and print it out\n",
    "knn.score(X_wine_test, y_wine_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned all about preprocessing we'll try these techniques out on a dataset that records information on UFO sightings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UFOs and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking column types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the UFO dataset's column types using the `dtypes` attribute. One column jumps out for transformation: the `date` column, which can be transformed into the `datetime` type. That will make our feature engineering efforts easier later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.669462Z",
     "start_time": "2020-02-16T17:50:55.626070Z"
    }
   },
   "outputs": [],
   "source": [
    "ufo = pd.read_csv('data/ufo_sightings_large.csv')\n",
    "ufo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:55.685420Z",
     "start_time": "2020-02-16T17:50:55.671459Z"
    }
   },
   "outputs": [],
   "source": [
    "ufo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.091209Z",
     "start_time": "2020-02-16T17:50:55.687414Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change the date column to type datetime\n",
    "ufo[\"date\"] = pd.to_datetime(ufo[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove some of the rows where certain columns have missing values. We're going to look at the `length_of_time column`, the `state` column, and the `type` column. If any of the values in these columns are missing, we're going to drop the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.107200Z",
     "start_time": "2020-02-16T17:50:56.093204Z"
    }
   },
   "outputs": [],
   "source": [
    "ufo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.122162Z",
     "start_time": "2020-02-16T17:50:56.109163Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check how many values are missing in the length_of_time, state, and type columns\n",
    "ufo[['length_of_time', 'state', 'type']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.138110Z",
     "start_time": "2020-02-16T17:50:56.124146Z"
    }
   },
   "outputs": [],
   "source": [
    "# Keep only rows where length_of_time, state, and type are not null\n",
    "ufo = ufo[ufo[\"length_of_time\"].notnull() & ufo[\"state\"].notnull() & ufo[\"type\"].notnull()]\n",
    "ufo.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.154063Z",
     "start_time": "2020-02-16T17:50:56.141076Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the shape of the new dataset\n",
    "ufo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables and standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting numbers from strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `length_of_time` field in the UFO dataset is a text field that has the number of minutes within the string. Here, we'll extract that number from that text field using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.170027Z",
     "start_time": "2020-02-16T17:50:56.158033Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def return_minutes(time_string):\n",
    "    # We'll use \\d+ to grab digits and match it to the column values\n",
    "    pattern = re.compile(r\"(\\d+|\\d{1,2}\\.\\d{1,2})(?:[^0-9\\.]*)?(?:minutes*|mins*)|(\\d+):(?:\\d*?)\")    \n",
    "        \n",
    "    # Use match on the pattern and column\n",
    "    num = re.search(pattern, time_string)\n",
    "    if num is not None:\n",
    "        return math.floor(float((num.group(1) if num.group(1) is not None else num.group(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.199953Z",
     "start_time": "2020-02-16T17:50:56.171995Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the extraction to the length_of_time column\n",
    "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.215879Z",
     "start_time": "2020-02-16T17:50:56.201915Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take a look at the head of both of the columns\n",
    "ufo[[\"length_of_time\", \"minutes\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we end up with some `NaN`s in the DataFrame. That's okay for now; we'll take care of those before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.231858Z",
     "start_time": "2020-02-16T17:50:56.217876Z"
    }
   },
   "outputs": [],
   "source": [
    "ufo = ufo[(ufo['seconds'] != 0.0) & ufo['minutes'].notnull()]\n",
    "ufo.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying features for standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll investigate the variance of columns in the UFO dataset to determine which features should be standardized. After taking a look at the variances of the `seconds` and `minutes` column, we'll see that the variance of the seconds column is extremely high. Because `seconds` and `minutes` are related to each other (an issue we'll deal with when we select features for modeling), let's log normlize the `seconds` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.263782Z",
     "start_time": "2020-02-16T17:50:56.233830Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the variance of the columns\n",
    "ufo.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.279738Z",
     "start_time": "2020-02-16T17:50:56.265748Z"
    }
   },
   "outputs": [],
   "source": [
    "# Log normalize the seconds column\n",
    "ufo[\"seconds_log\"] = np.log(ufo[\"seconds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.295667Z",
     "start_time": "2020-02-16T17:50:56.281702Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the variance of just the seconds_log column\n",
    "print(ufo[\"seconds_log\"].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are couple of columns in the UFO dataset that need to be encoded before they can be modeled through scikit-learn. We'll do that transformation here, using both binary and one-hot encoding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.310663Z",
     "start_time": "2020-02-16T17:50:56.296695Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use Pandas to encode us values as 1 and others as 0\n",
    "ufo[\"country_enc\"] = ufo[\"country\"].apply(lambda val: 1 if val == \"us\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.326581Z",
     "start_time": "2020-02-16T17:50:56.312619Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the number of unique type values\n",
    "print(len(ufo[\"type\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.341574Z",
     "start_time": "2020-02-16T17:50:56.329575Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a one-hot encoded set of the type values\n",
    "type_set = pd.get_dummies(ufo[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.357523Z",
     "start_time": "2020-02-16T17:50:56.344532Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate this set back to the ufo DataFrame\n",
    "ufo = pd.concat([ufo, type_set], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.389447Z",
     "start_time": "2020-02-16T17:50:56.359495Z"
    }
   },
   "outputs": [],
   "source": [
    "ufo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features from dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another feature engineering task to perform is month and year extraction. We'll perform this task on the `date` column of the `ufo` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.405397Z",
     "start_time": "2020-02-16T17:50:56.391427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the first 5 rows of the date column\n",
    "ufo[\"date\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.421360Z",
     "start_time": "2020-02-16T17:50:56.407367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the month from the date column\n",
    "ufo[\"month\"] = ufo[\"date\"].apply(lambda row: row.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.437286Z",
     "start_time": "2020-02-16T17:50:56.426317Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the year from the date column\n",
    "ufo[\"year\"] = ufo[\"date\"].apply(lambda row: row.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.453275Z",
     "start_time": "2020-02-16T17:50:56.440278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the head of all three columns\n",
    "ufo[[\"date\", \"month\", \"year\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the `desc` column in the UFO dataset into tf/idf vectors, since there's likely something we can learn from this field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.469225Z",
     "start_time": "2020-02-16T17:50:56.456234Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the head of the desc field\n",
    "ufo[\"desc\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.485157Z",
     "start_time": "2020-02-16T17:50:56.471195Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the tfidf vectorizer object\n",
    "vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.532060Z",
     "start_time": "2020-02-16T17:50:56.487151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use vec's fit_transform method on the desc field\n",
    "desc_tfidf = vec.fit_transform(ufo[\"desc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.547017Z",
     "start_time": "2020-02-16T17:50:56.533031Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the number of columns this creates.\n",
    "desc_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text vector has a large number of columns. We'll work on selecting the features we want to use for modeling in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection and modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the ideal dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of some of the unnecessary features. Because we have an encoded country column, `country_enc`, we keep it and drop other columns related to location: `city`, `country`, `lat`, `long`, `state`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have columns related to `month` and `year`, so we don't need the `date` or `recorded` columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We vectorized `desc`, so we don't need it anymore. For now we'll keep `type`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll keep `seconds_log` and drop `seconds` and `minutes`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get rid of the `length_of_time` column, which is unnecessary after extracting `minutes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.561977Z",
     "start_time": "2020-02-16T17:50:56.548987Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the correlation between the seconds, seconds_log, and minutes columns\n",
    "ufo[[\"seconds\", \"seconds_log\", \"minutes\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.577936Z",
     "start_time": "2020-02-16T17:50:56.563947Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a list of features to drop   \n",
    "to_drop = [\"city\", \"country\", \"date\", \"desc\", \"lat\", \"length_of_time\", \"long\", \"minutes\", \"recorded\", \"seconds\", \"state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:56.592896Z",
     "start_time": "2020-02-16T17:50:56.580905Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop those features\n",
    "ufo = ufo.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:58.966940Z",
     "start_time": "2020-02-16T17:50:56.604840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's also filter some words out of the text vector we created\n",
    "vocab = {v:k for k,v in vec.vocabulary_.items()}\n",
    "filtered_words = words_to_filter(vocab, vec.vocabulary_, desc_tfidf, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost done. In the next examples, we'll try modeling the UFO data in a couple of different ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling the UFO dataset, part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're going to build a k-nearest neighbor model to predict which country the UFO sighting took place in. Our `X` dataset has the log-normalized seconds column, the one-hot encoded type columns, as well as the month and year when the sighting took place. The `y` labels are the encoded country column, where 1 is `us` and 0 is `ca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:58.982898Z",
     "start_time": "2020-02-16T17:50:58.969932Z"
    }
   },
   "outputs": [],
   "source": [
    "X = ufo.drop(['type', 'country_enc'], axis=1)\n",
    "y = ufo['country_enc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:58.997882Z",
     "start_time": "2020-02-16T17:50:58.985888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take a look at the features in the X set of data\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:59.013846Z",
     "start_time": "2020-02-16T17:50:59.000848Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the X and y sets using train_test_split, setting stratify=y\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:59.043760Z",
     "start_time": "2020-02-16T17:50:59.016821Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit knn to the training sets\n",
    "knn.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:59.074661Z",
     "start_time": "2020-02-16T17:50:59.045742Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the score of knn on the test sets\n",
    "print(knn.score(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs pretty well. It seems like we've made pretty good feature selection choices here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling the UFO dataset, part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's build a model using the text vector we created, `desc_tfidf`, using the `filtered_words` list to create a filtered text vector. Let's see if we can predict the `type` of the sighting based on the text. We'll use a Naive Bayes model for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:59.089637Z",
     "start_time": "2020-02-16T17:50:59.077644Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the list of filtered words we created to filter the text vector\n",
    "filtered_text = desc_tfidf[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:59.183039Z",
     "start_time": "2020-02-16T17:50:59.093603Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the X and y sets using train_test_split, setting stratify=y\n",
    "y = ufo['type']\n",
    "train_X, test_X, train_y, test_y = train_test_split(filtered_text.toarray(), y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:50:59.336664Z",
     "start_time": "2020-02-16T17:50:59.185032Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit nb to the training sets\n",
    "nb.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-16T17:51:00.037493Z",
     "start_time": "2020-02-16T17:50:59.338624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the score of nb on the test sets\n",
    "nb.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this model performs very poorly on this text data. This is a clear case where iteration would be necessary to figure out what subset of text improves the model, and if perhaps any of the other features are useful in predicting `type`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**[Week 5 - Data Preprocessing and Hyperparameter Tuning](https://radu-enuca.gitbook.io/ml-challenge/preprocessing-and-tuning)**\n",
    "\n",
    "*Have questions or comments? Visit the ML Challenge Mattermost Channel.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "393.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
